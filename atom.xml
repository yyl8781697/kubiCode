<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Kubi Code'Blog]]></title>
  <subtitle><![CDATA[The palest ink is better than the best memory.]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://kubicode.me/"/>
  <updated>2016-06-04T07:18:23.000Z</updated>
  <id>http://kubicode.me/</id>
  
  <author>
    <name><![CDATA[Kubi Code]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[RankNet:基于梯度下降的学习排序]]></title>
    <link href="http://kubicode.me/2016/05/30/Machine%20Learning/RankNet-Learning-to-Rank-using-Gradient-Descent/"/>
    <id>http://kubicode.me/2016/05/30/Machine Learning/RankNet-Learning-to-Rank-using-Gradient-Descent/</id>
    <published>2016-05-30T11:51:21.000Z</published>
    <updated>2016-06-04T07:18:23.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p><code>RankNet</code>是一种基于<code>pairwise</code>的学习排序,假设文档<code>A</code>以<code>P</code>的概率排在文档<code>B</code>的前面，则<code>RankNet</code>就是使用神经网络算法来使得这个概率最大化.</p>
</blockquote>
<h2 id="算法原理">算法原理</h2><p>假设现在有一对文档$D_i$和$D_j$,给定一个目标概率$\bar{P}_{i,j}$,以表示文档$D_i$将会排在文档$D_j$的前面.</p>
<ol>
<li>如果$\bar{P}_{i,j}=1$,表示$D_i$一定会排在$D_j$前面</li>
<li>如果$\bar{P}_{i,j}=0.5$,表示$D_i$和$D_j$的前后关系将无法确定</li>
<li>如果$\bar{P}_{i,j}=0$,表示$D_i$一定不会排在$D_j$前面</li>
</ol>
<p>现在有一个实值的排序函数$f$,如果$f(x_i) &gt; f(x_j)$，则会有$D_i \triangleright D_j$(表示$D_i$会排在$D_j$前面)</p>
<blockquote>
<p>这里$x$是为文档$D$所表示的特征向量</p>
</blockquote>
<a id="more"></a>
<p>我们现在将$P(D_i \triangleright D_j)$前后顺序的预测概率表示为$P_{i,j}$,同时定义$o_i \equiv f(x_i)$ 以及 $o_{i,j}=f(x_i)-f(x_j)$，则我们可以<code>logistic</code>函数来表示$P_{i,j}$:<br>$$P_{i,j} \equiv \frac{e^{o_{i,j}}}{1+e^{o_{i,j}}} \equiv \frac{1}{1+e^{-o_{i,j}}}$$</p>
<p>为了衡量预测概率$P_{i,j}$与期望/目标概率$\bar{P}_{i,j}$的接近程度，这里使用<code>Cross Entropy</code>作为损失函数:<br>$$C_{i,j} = -\bar{P}_{i,j} log P_{i,j} - (1-\bar{P}_{i,j}) log (1-P_{i,j})$$</p>
<p>将$P_{i,j}$代入损失函数$C_{i,j}$之后即可得到:<br>$$\begin{equation}\begin{split}C_{i,j}&amp;=-\bar{P}_{i,j} log \frac{e^{o_{i,j}}}{1+e^{o_{i,j}}} - (1-\bar{P}_{i,j}) log (1-\frac{e^{o_{i,j}}}{1+e^{o_{i,j}}}) \\<br>&amp;= -\bar{P}_{i,j} log \frac{e^{o_{i,j}}}{1+e^{o_{i,j}}} - (1-\bar{P}_{i,j}) log (\frac{1}{1+e^{o_{i,j}}}) \\<br>&amp;= -\bar{P}_{i,j} \left( log \frac{e^{o_{i,j}}}{1+e^{o_{i,j}}} - log (\frac{1}{1+e^{o_{i,j}}}) \right) - log (\frac{1}{1+e^{o_{i,j}}})  \\<br>&amp;= -\bar{P}_{i,j}  o_{i,j} + log(1+e^{o_{i,j}})<br>\end{split}\end{equation}$$</p>
<p>下面的图是当$\bar{P}_{i,j} \in \{0,0.5,1\}$时的损失函数情况:<br><img src="/img/RankNet-Learning-to-Rank-using-Gradient-Descent/lossfunction.png" style="align:center;margin:0 auto" width="400px"></p>
<p>当$\bar{P}_{i,j} = 1$的时候，<code>Cross Entropy</code>的损失函数将会变为:$$C_{i,j}=log(1+e^{-o_{i,j}})$$<br>直接会变为一个<code>log</code>型的损失函数，其中$o_i-o_j$越大，损失函数的值也就会越小，这也是我们所期望训练的结果(表示我们的样本全部成立)</p>
<p>现我们损失函数$C_{i,j}$求$o$的偏导:<br>$$\frac{ \partial{C}}{ \partial{o_i}} = \left( -\bar{P}_{i,j}+\frac{e^{o_i-o_j}}{1+e^{o_i-o_j}} \right) =\left( -\bar{P}_{i,j}+P_{i,j} \right) = -\frac{ \partial{C}}{ \partial{o_j}}$$</p>
<blockquote>
<p>其实可以发现就是在$-\bar{P}_{i,j}+P_{i,j} $时就是我们的目标</p>
</blockquote>
<p>现我们认为$w$为$o=f(x:w)$的一个权重,也就是我们最终希望求解的值，而这个参数我们就可以使用<code>随机梯度下降法来求解</code>:<br>$$w_k \rightarrow  w_k - \eta \frac{\partial{C}}{\partial{w_k}} = w_k - \eta \left( \frac{\partial{C}}{\partial{o_i}} \frac{\partial{o_i}}{\partial{w_k}} + \frac{\partial{C}}{\partial{o_j}} \frac{\partial{o_j}}{\partial{w_k}} \right)$$</p>
<p>其中$\eta$表示学习率，一般取一个比较小的数（比如:$1e-3$,$1e-5$）<br>另外我们可以求得$C$的增量变化:<br>$$\Delta C = \sum_k \frac{\partial{C}}{\partial{w_k}} \Delta w_k = \sum_k \frac{\partial{C}}{\partial{w_k}} \left( - \eta \frac{\partial{C}}{\partial{w_k}} \right) = - \eta \sum_k \left( \frac{\partial{C}}{\partial{w_k}} \right)^2  &lt; 0 $$</p>
<ol>
<li>$\Delta C &lt; 0 $表示随着权重参数$w$的变化，损失函数$C$会越来越小</li>
<li>另外当梯度$\frac{\partial{C}}{\partial{w_k}} = 0$时，才会让损失函数达到最小值</li>
</ol>
<p>上面的式子告诉我们通过梯度下降法求解<code>RankNet</code>时，就算<code>算分</code>函数没有好的梯度或者不可求导时任可以进行权重的更新（因为最终可以通过损失函数和权重$w$进行求），但是有的要求权重$w$与最终的算法$o$是相关的。</p>
<h2 id="合并概率">合并概率</h2><p>理想的情况下，$\bar{o}$的输出得到的模型应该是这样纸的:$$\bar{P}_{i,j}=\frac{e^{\bar{o}_{i,j}}}{1+e^{\bar{o}_{i,j}}}$$</p>
<blockquote>
<p>其中$\bar{o}_{i,j}=\bar{o}_i-\bar{o}_j$</p>
</blockquote>
<p>上面的模型需要$\bar{P}_{i,j}$保持一致性，也就是如果$D_i$的相关性要高于$D_j$,$D_j$的相关性同时也是要高于$D_k$，则$D_i$的相关性也是一定要高于$D_j$，如果没有保持一致性，其实上面的理论就不好使了。。。<br>现给定$\bar{P}_{i,j}$和$\bar{P}_{j,k}$时会有:</p>
<p>$$\begin{equation}\begin{split} \bar{P}_{i,k}&amp;= \frac{e^{\bar{o}_{i,k}}}{1+e^{\bar{o}_{i,k}}}\\<br>&amp;= \frac{e^{\bar{o}_{i,j}e^\bar{o}_{j,k}}}{1+e^{\bar{o}_{i,j}e^\bar{o}_{j,k}}} \\<br>&amp;= \frac{ \frac{e^{\bar{o}_{i,j}e^\bar{o}_{j,k}}}{(1+e^{\bar{o}_{i,j}})(1+e^{\bar{o}_{j,k}})} }{ \frac{1+e^{\bar{o}_{i,j}e^\bar{o}_{j,k}}}{(1+e^{\bar{o}_{i,j}})(1+e^{\bar{o}_{j,k}})}} \\<br>&amp;= \frac{\bar{P}_{i,j} \bar{P}_{j,k}}{\frac{(1+e^{\bar{o}_{i,j}}+e^{\bar{o}_{j,k}}+e^{\bar{o}_{i,j}}e^{\bar{o}_{j,k}})+(2e^{\bar{o}_{i,j}}e^{\bar{o}_{j,k}})-(e^{\bar{o}_{i,j}}+e^{\bar{o}_{i,j}}e^{\bar{o}_{j,k}})-(e^{\bar{o}_{j,k}}+e^{\bar{o}_{i,j}}e^{\bar{o}_{j,k}})}{(1+e^{\bar{o}_{i,j}})(1+e^{\bar{o}_{j,k}})}} \\<br>&amp;= \frac{\bar{P}_{i,j} \bar{P}_{j,k}}{1+2\bar{P}_{i,j}\bar{P}_{j,k}-\bar{P}_{i,j}-\bar{P}_{j,k}}<br>\end{split}\end{equation}$$</p>
<blockquote>
<p>其中第一步是基于这个来的:$$\begin{equation}\begin{split}e^{\bar{o}_{i,k}}&amp;=e^{\bar{o}_i-\bar{o}_k} \\<br>&amp;= e^{\bar{o}_i-\bar{o}_j+\bar{o}_j-\bar{o}_k}\\<br>&amp;= e^{\bar{o}_{i,j}+\bar{o}_{j,k}}  \\<br>&amp;= e^{\bar{o}_{i,j}}e^{\bar{o}_{j,k}}<br>\end{split}\end{equation}$$</p>
</blockquote>
<p>当$\bar{P}_{i,j}=\bar{P}_{i,k}=P$时，其$\bar{P}_{i,k}$的取值情况为:<br><img src="/img/RankNet-Learning-to-Rank-using-Gradient-Descent/pik.png" width="400px"></p>
<ol>
<li>$P=0$时，有$\bar{P}_{i,k}=P=0$ 表示:$D_i$排$D_j$后面,$D_j$排$D_j$的后面，则$D_i$也一定排$D_j$的后面</li>
<li>$0 &lt; P &lt; 0.5$时，$\bar{P}_{i,k} &lt; P$</li>
<li>$P=0.5$时，有$\bar{P}_{i,k}=P=0.5$ 表示:$D_i$有一般概率排$D_j$前面,$D_j$也有一半的概率排$D_j$的前面，则$D_i$同样也是一半的概率排$D_j$的前面</li>
<li>$0.5 &lt; P &lt; 1$时，$\bar{P}_{i,k} &gt; P$</li>
<li>$P=1$时，有$\bar{P}_{i,k}=P=1$ 表示:$D_i$排$D_j$前面,$D_j$排$D_j$的前面，则$D_i$也一定排$D_j$的前面</li>
</ol>
<blockquote>
<p>从上面的图中可以看到，其实目标概率是都可以保持一致性的.</p>
</blockquote>
<h2 id="神经网络训练">神经网络训练</h2><p><code>RankNet</code>使用的是一个2层的神经网络作为算分模型$f(x:w,b)$,他在排序分数的公式是:<br>$$o = f(x:w,b) = f^{(2)}  \left( \sum_l w_l^{(2)} \cdot f^{(1)}  \left( \sum_k w_{lk}^{(1)}x_k +b^{(1)} \right) +b^{(2)} \right)$$</p>
<p>其中:</p>
<ol>
<li>$x_k$表示输入的$k$个特征元素</li>
<li>$w$表示每一层的权重,$b$表示每一层的偏置，上标$(\cdot)$表示当前所属的神经网络的层数</li>
<li>下标$l$表示第一层的单元数量</li>
<li>$f$使用<code>sigmoid</code>作为激活函数</li>
</ol>
<p>这里我们输入的样本是<code>pair</code>对$\{(x_i,x_j),\bar{P}_{i,j}\}$,其中$\bar{P}_{i,j}$就是我们的目标概率,根据第一小节(算法原理)中指到的，使用梯度下降法进行求解:<br>$$\begin{equation}\begin{split} \frac{\partial{C}}{\partial{w}} &amp;= \frac{\partial{C}}{\partial{o_i}} \frac{\partial{o_i}}{\partial{w_k}} + \frac{\partial{C}}{\partial{o_j}} \frac{\partial{o_j}}{\partial{w_k}}  \\<br>&amp;=  \left( -\bar{P}_{i,j}+\frac{e^{o_i-o_j}}{1+e^{o_i-o_j}} \right) \left( \frac{\partial{o_i}}{\partial{w_k}} - \frac{\partial{o_j}}{\partial{w_k}}\right)\\<br>&amp;=  \lambda_{i,j} \left( \frac{\partial{o_i}}{\partial{w_k}} - \frac{\partial{o_j}}{\partial{w_k}}\right) \\<br>\end{split}\end{equation}$$<br>$ $</p>
<blockquote>
<p>记$\lambda_{i,j} =  \frac{\partial{C}}{\partial{o_i}}  = -\frac{\partial{C}}{\partial{o_j}} \left( -\bar{P}_{i,j}+\frac{e^{o_i-o_j}}{1+e^{o_i-o_j}} \right) $</p>
</blockquote>
<p>在对于上面的二层神经网络求解时:<br>$$\frac{\partial{C}}{\partial{b^{(2)}}} = \lambda_{i,j}({f’}_i^{(2)}-{f’}_i^{(2)}) = \Delta_i^{(2)} - \Delta_j^{(2)} \\<br>\frac{\partial{C}}{\partial{w^{(2)}}} = \Delta_i^{(2)}f_i^{(1)}  - \Delta_j^{(2)}f_j^{(1)} \\<br>\frac{\partial{C}}{\partial{b^{(1)}}} = \Delta_i^{(2)}f_i^{(1)}w^{(2)}   - \Delta_j^{(2)}f_j^{(1)}w^{(2)} \\<br>\frac{\partial{C}}{\partial{w_k^{(1)}}} = \Delta_i^{(2)} x_{i,k} - \Delta_j^{(2)} x_{j,k}<br>$$<br>这样神经网络就可以使用<code>前向预测</code>和<code>后向反馈</code>来进行训练了,只是其后向反馈阶段是需要通过<code>pair</code>进行计算的。</p>
<h2 id="神经网络加速">神经网络加速</h2><p>上面的是对于每一对<code>pair</code>都会进行一次权重的更新，其实是可以对同一个排序下的所有文档<code>pair</code>全部带入神经网络进行前向预测，然后计算总差分并进行误差后向反馈，这样将大大减少误差反向传播的次数，其更新的公式为:<br>$$\Delta w = -\eta \sum_{\{i,j\}\in I}  \left(\lambda_{i,j} \frac{\partial{o_i}}{\partial{w_k}} - \lambda_{i,j} \frac{\partial{o_j}}{\partial{w_k}}\right) = -\eta \sum_i \lambda_i \frac{\partial{o_i}}{\partial{w_k}} $$<br>其中:<br>$$\lambda_i = \sum_{j:\{i,j\} \in I} \lambda_{i,j} - \sum_{j:\{j,i\} \in I} \lambda_{i,j}$$</p>
<blockquote>
<p>$I$表示$\{i,j\}$两个文档满足$D_i \triangleright D_j$,也就是$\bar{P}_{i,j}=1$</p>
</blockquote>
<p>这种方式可以看做是一种mini-batch的梯度下降算法，可以大大加快原始的神经网络训练.</p>
<h2 id="总结">总结</h2><p><code>RankNet</code>训练希望文档<code>pair</code>对的前后排序概率与目标概率一致，用交叉熵作为损失函数，在实际排序中使用了神经网络作为算分排序函数，同时可以有<code>min-batch</code>的批量训练方法。据说微软的<code>Bing</code>之前使用着他.</p>
<h2 id="参考">参考</h2><ol>
<li>Burges, Chris, et al. “Learning to rank using gradient descent.” Proceedings of the 22nd international conference on Machine learning. ACM, 2005.</li>
<li>Burges, Christopher JC. “From ranknet to lambdarank to lambdamart: An overview.” Learning 11 (2010): 23-581.</li>
<li>Li, Hang. “Learning to rank for information retrieval and natural language processing.” Synthesis Lectures on Human Language Technologies 7.3 (2014): 1-121.</li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p><code>RankNet</code>是一种基于<code>pairwise</code>的学习排序,假设文档<code>A</code>以<code>P</code>的概率排在文档<code>B</code>的前面，则<code>RankNet</code>就是使用神经网络算法来使得这个概率最大化.</p>
</blockquote>
<h2 id="算法原理">算法原理</h2><p>假设现在有一对文档$D_i$和$D_j$,给定一个目标概率$\bar{P}_{i,j}$,以表示文档$D_i$将会排在文档$D_j$的前面.</p>
<ol>
<li>如果$\bar{P}_{i,j}=1$,表示$D_i$一定会排在$D_j$前面</li>
<li>如果$\bar{P}_{i,j}=0.5$,表示$D_i$和$D_j$的前后关系将无法确定</li>
<li>如果$\bar{P}_{i,j}=0$,表示$D_i$一定不会排在$D_j$前面</li>
</ol>
<p>现在有一个实值的排序函数$f$,如果$f(x_i) &gt; f(x_j)$，则会有$D_i \triangleright D_j$(表示$D_i$会排在$D_j$前面)</p>
<blockquote>
<p>这里$x$是为文档$D$所表示的特征向量</p>
</blockquote>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[GBRank:一种基于回归的学习排序算法]]></title>
    <link href="http://kubicode.me/2016/05/08/Machine%20Learning/GBRank-A-PairWsie-LTR-Base-on-Regression-Framework/"/>
    <id>http://kubicode.me/2016/05/08/Machine Learning/GBRank-A-PairWsie-LTR-Base-on-Regression-Framework/</id>
    <published>2016-05-08T09:30:39.000Z</published>
    <updated>2016-05-08T15:27:47.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p><code>GBRank</code>是一种<code>pairwise</code>的学习排序算法，他是基于回归来解决<code>pair</code>对的先后排序问题。在<code>GBRank</code>中，使用的回归算法是<code>GBT(Gradient Boosting Tree)</code>，可以参考<a href="http://kubicode.me/2016/04/24/Machine%20Learning/From-Gradient-Boosting-to-GBT/#Gradient_tree_boosting" target="_blank" rel="external">这个</a>，<code>pairwise</code>相关的可以参考<a href="http://kubicode.me/2016/04/10/Machine%20Learning/LTR-Pairwise-Study/" target="_blank" rel="external">这个</a></p>
</blockquote>
<h2 id="算法原理">算法原理</h2><p>一般来说在搜索引擎里面，相关性越高的越应该排在前面。现在<code>query-doc</code>的特征使用向量$x$或者$y$表示，假设现在有一个文档对$\left \langle x_i,y_i \right \rangle$，当$x_i$排在$y_i$前面时，我们使用$x_i \succ y_i$来表示。</p>
<p>我们含顺序的<code>pair</code>对用如下集合表示(也就是真的$x_i$真的排在$y_i$前面):$$S=\{ \left \langle x_i,y_i \right \rangle | x_i \succ y_i,i = 1…N \}$$</p>
<p>现假设学习的排序函数为$h$，我们希望当$h(x_i)&gt;h(y_i)$时，满足$x_i \succ y_i$的数量越多越好.<br>现在将$h$的风险函数用如下式子表示:$$R(h) = \frac{1}{2} \sum_{i=1}^N \left( max\{0,h(y_i)-h(x_i)\} \right)^2$$<br><a id="more"></a></p>
<p>从$R(h)$可以知道知道每个<code>pair</code>对$\left \langle x_i,y_i \right \rangle$的cost为:</p>
<p><center><img src="/img/GBRank-A-PairWsie-LTR-Base-on-Regression-Framework/cost_function.png" width="400px"></center><br>可以发现当:</p>
<ol>
<li>$h(x_i) \geq h(y_i)$，cost代价为0，也就是并不会对最终的风险函数的值产生影响</li>
<li>$h(x_i) &lt; h(y_i)$，cost代价为其差值的平方</li>
</ol>
<p>上述风险函数直接优化比较困难，这里一个巧妙的解决方案时使用回归的方法，也就是$x_i$或者$y_i$去拟合他们另个预测值目标。<br>为了避免优化函数$h$是一个常量，风险函数一般情况下会加上一个平滑项$\tau$($0 &lt; \tau \leq 1$)：$$R(h,\tau ) = \frac{1}{2} \sum_{i=1}^N \left( max\{0,h(y_i)-h(x_i)+\tau \} \right)^2 -\lambda \tau^2 $$</p>
<blockquote>
<p>因为当$h$为常量函数时，先前的$R(h)=0$就没有再优化的空间了<br>其实加了平滑项就变相的转为:如果希望$x_i \succ y_i$，就得有$h(x_i) &gt; h(y_i)+\tau$，也就是更为严格了，多了一个<code>gap</code></p>
</blockquote>
<p>对于$R(h)$计算$h(x_i)$和$h(y_i)$的负梯度为:$$max\{0,h(y_i)-h(x_i)\} \quad,\quad -max\{0,h(y_i)-h(x_i)\}$$<br>可以发现当<code>pair</code>对符合$\left \langle x_i,y_i \right \rangle$的顺序时，上述的梯度均为0，对于这类<code>case</code>就没有必要在去优化了(以为已经满足目标了)，但是对于另一类，如果$h$不满足<code>pair</code>$\left \langle x_i,y_i \right \rangle$，他们对应的梯度为:$$h(y_i)-h(x_i) \quad,\quad h(x_i)-h(y_i)$$</p>
<blockquote>
<p>因为此时$h(y_i)&gt;h(x_i)$<br>还有对于上面两个梯度的后面那个式子存在的意义不是很理解-_-</p>
</blockquote>
<p>到了这儿，我们知道所谓的训练样本就是对于$x_i \succ y_i$但是$h(y_i) &gt; h(x_i)$，并且使用的是回归方法，<code>GBRank</code>为其巧妙的找了训练的目标:$x_i$的目标为$h(y_i)+ \tau$以及$y_i$的目标为$h(x_i)- \tau$，也就是在每次迭代是将会构建以下训练集:$$\{(x_i,h(y_i)+\tau),(y_i,h(x_i)-\tau)\}$$</p>
<h2 id="算法步骤">算法步骤</h2><p><code>GBRank</code>使用<code>GBT</code>为回归函数，所以整个<code>GBRank</code>的算法过程为:<br><strong>Input</strong>:</p>
<ul>
<li>训练数据集,真实的排序pair对$S=\{(x_1,y_1),(x_2,y_2)…(x_N,y_N)\}$</li>
<li>迭代的次数:$K$</li>
</ul>
<p><strong>Output</strong>:</p>
<ul>
<li>最终模型$h(x)$</li>
</ul>
<p><strong>Procedure</strong>:</p>
<ol>
<li>随机初始化为一个函数$h_o$</li>
<li>循环$k \in \{1….K\}$<ol>
<li>使用$h_{k-1}$作为近似的$h$函数,我们将对样本的计算结果分到两个不相交的集合:$$S^+=\{\left \langle x_i,y_i \right \rangle \in S | h_{k-1}(x_i) \geq h_{k-1}(y_i)+\tau \}$$和$$S^-=\{\left \langle x_i,y_i \right \rangle \in S | h_{k-1}(x_i) &lt; h_{k-1}(y_i)+\tau \}$$</li>
<li>使用<code>GBT</code>对下面的数据集拟合一个回归函数$g(x)$:$$\{(x_i,h_{k-1}(y_i)+\tau),(y_i,h_{k-1}(x_i)-\tau) | (x_i,y_i) \in S^- \}$$</li>
<li>进行模型的更新:$$h_k(x) = \frac{kh_{k-1}(x)+\eta g_k(x)}{k+1}$$其中$\eta$为收缩因子</li>
</ol>
</li>
<li>输出最终的模型$h_k(x)$</li>
</ol>
<blockquote>
<p>注意了，其实这里的回归函数还可以使用其他的回归函数来代替，比如$Linear Regression$之类的</p>
</blockquote>
<h2 id="总结">总结</h2><p>$h(x)$为最终的排序函数，<code>GBRank</code>在训练时每次迭代中将$h_k(x)$的排序结果与真实结果不一样的样本（就是分错的样本）单独拿出来做训练样本，并且其训练目标为<code>pair</code>的另一个预测值作为回归目标，非常巧妙。<br>此时重新看这个<code>GBRank</code>模型与<code>AdaBoost</code>、<code>GBT</code>其实很大同小异，都是将上一次训练中分错的样本再拿来训练，也是一个提升的模型</p>
<p>在其paper中的实验结果也是要略好于$RankSvm$，但是其比较疼的时其训练还是比较复杂的，或者说比较耗时,其预测也会比较麻烦一点，所以使用时得慎重~</p>
<h2 id="参考">参考</h2><p>[1]. 2007-GBRank-A Regression Framework for Learning Ranking Functions Using Relative Relevance Judgments</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p><code>GBRank</code>是一种<code>pairwise</code>的学习排序算法，他是基于回归来解决<code>pair</code>对的先后排序问题。在<code>GBRank</code>中，使用的回归算法是<code>GBT(Gradient Boosting Tree)</code>，可以参考<a href="http://kubicode.me/2016/04/24/Machine%20Learning/From-Gradient-Boosting-to-GBT/#Gradient_tree_boosting">这个</a>，<code>pairwise</code>相关的可以参考<a href="http://kubicode.me/2016/04/10/Machine%20Learning/LTR-Pairwise-Study/">这个</a></p>
</blockquote>
<h2 id="算法原理">算法原理</h2><p>一般来说在搜索引擎里面，相关性越高的越应该排在前面。现在<code>query-doc</code>的特征使用向量$x$或者$y$表示，假设现在有一个文档对$\left \langle x_i,y_i \right \rangle$，当$x_i$排在$y_i$前面时，我们使用$x_i \succ y_i$来表示。</p>
<p>我们含顺序的<code>pair</code>对用如下集合表示(也就是真的$x_i$真的排在$y_i$前面):$$S=\{ \left \langle x_i,y_i \right \rangle | x_i \succ y_i,i = 1…N \}$$</p>
<p>现假设学习的排序函数为$h$，我们希望当$h(x_i)&gt;h(y_i)$时，满足$x_i \succ y_i$的数量越多越好.<br>现在将$h$的风险函数用如下式子表示:$$R(h) = \frac{1}{2} \sum_{i=1}^N \left( max\{0,h(y_i)-h(x_i)\} \right)^2$$<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[从Gradient Boosting 到GBT]]></title>
    <link href="http://kubicode.me/2016/04/24/Machine%20Learning/From-Gradient-Boosting-to-GBT/"/>
    <id>http://kubicode.me/2016/04/24/Machine Learning/From-Gradient-Boosting-to-GBT/</id>
    <published>2016-04-24T15:51:08.000Z</published>
    <updated>2016-05-02T15:48:21.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大部分参考(译)自wiki[1]<br>如果说<code>Gradient Boosting</code>是一种机器学习算法框架的话，我想<code>GBT(Gradient Boosting Tree)</code>看做它的实现更为合适</p>
</blockquote>
<h2 id="Gradient_Boosting原理">Gradient Boosting原理</h2><blockquote>
<p>与其他<code>Boosting</code>方法一样，<code>Gradient Boosting</code>通过迭代将弱分类器合并成一个强分类器的方法。</p>
</blockquote>
<p>对于标准的$(x_i,y_i)$训练集，<code>Gradient Boosting</code>在迭代到第$m$次时，可能会得到一个分类能力不是很强的$f_m$模型，但是他下次迭代并不改变$f_m$，而是生成一个这样的新模型:$$f_{m+1} = f_m+G_{m+1}$$使得$f_{m+1}$较$f_m$而言拥有更为强大的分类能力，那么问题来了,这个$G_{m+1}$该如何训练呢?<br><a id="more"></a></p>
<p>现在假如训练完$G_{m+1}$可以得到完美的$f_{m+1}$，那么也就是有:$$f_{m+1} = f_m+G_{m+1}=y$$<br>这个式子可以写成$$G_{m+1}=y-f_m$$ 其中$y-f_m$表示上一次迭代得到分类器预测结果与真实结果的差值，我们一般称之为的<code>残差</code>(residual)，因此也可以理解为<code>Gradient Boosting</code>在每一轮训练新的分类器将会拟合<code>残差</code>进行最优化</p>
<h2 id="Gradient_Boosting算法">Gradient Boosting算法</h2><p>假设现有训练集$\left\{ (x_1,y_1),(x_2,y_2)…(x_n,y_n)  \right\}$，其中$x$是特征向量，$y$是相应的训练目标，在给定相应的损失函数$L(y,f(x))$,我们的目标是找到一个近似的函数$f(x)$使得与真实函数$f^*(x)$的损失最小期望值最接近:$$f^* = \underset{f}{argmin} E_{x,y} \left[ L(y,f(x)) \right]$$</p>
<p><code>Gradient Boosting</code>方法假设$y$是一个实值，而$f(x)$近似目标函数是一个弱分类器$G_m(x)$加权求和的形式$$f(x)=\sum_{m=1}^M \gamma_mG_m(x)+const$$ 根据经验风险最小化的原则，近似函数$f(x)$将会尝试对训练集上的平均损失函数进行最小化，它从一个常量函数进行起步，并且通过贪心的方式进行逐步优化<br>$$<br>f_0(x) = \underset{\gamma}{argmin} \sum_{i=1}^n L(y_i,\gamma) \\<br>f_m(x) = f_{m-1}(x) + \underset{G \in H}{argmin} \sum_{i=1}^n L \left(y_i,f_{m-1}(x_i)+G_m(x_i) \right)<br>$$<br>然而，对于$L$为任意的损失函数时,在选择每一步最佳的$G_m(x_i)$时将会很难优化。<br>这里使用最速下降法(<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="external">steepest descent</a>)来解决这个问题<br>,在使用这种方法时，对于损失函数$L(y,G)$不要将其看做一个函数，而是将其看做通过函数得到的值的向量$G(x_1),G(x_2)…G(x_n)$,那么这样的话我们就可以将模型的式子写成如下的等式:<br>$$<br>f_m(x) = f_{m-1}(x) - \gamma_m \sum_{i=1}^n \triangledown_G L \left( y_i,f_{m-1}(x_i) \right) \\<br>\gamma_m = \underset{\gamma}{argmin} \sum_{i=1}^n L \left ( y_i,f_{m-1}(x_i)-\gamma \frac{\partial L(y_i,f_{m-1}(x_i))}{\partial G(x_i)}  \right )<br>$$<br>上面第一个式子表示根据梯度的负方向进行更新,第二个式子表明了$\gamma$使用线性搜索进行计算。</p>
<p>下面就是具体的<code>gradient boosting</code>步骤:<br><strong>Input</strong>:</p>
<ul>
<li>训练数据集$T=\{(x_1,y_1),(x_2,y_2)…(x_N,y_N)\}$</li>
<li>可导的损失函数:$L(y,f(x))$</li>
<li>迭代的次数:$M$</li>
</ul>
<p><strong>Output</strong>:</p>
<ul>
<li>最终模型$f(x)$</li>
</ul>
<p><strong>Procedure</strong>:</p>
<ol>
<li>使用一个常量进行模型的初始化$$f_0(x) = \underset{\gamma}{argmin} \sum_{i=1}^n L(y_i,\gamma) $$</li>
<li>循环$m \in \{1….M\}$<ol>
<li>计算残差$$r_{im}=-\left[ \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}  \right]_{f(x_i)=f_{m-1}(x_i)} \quad i=1…n$$</li>
<li>使用训练集$\{ (x_i,r_{im}) \}$对弱分类器$G_m(x)$进行拟合</li>
<li>通过线性搜索进行乘子$\gamma_m$的计算$$\gamma_m = \underset{\gamma}{argmin} \sum_{i=1}^n L \left(y_i,f_{m-1}(x_i)+\gamma_m G_m(x_i) \right)$$</li>
<li>进行模型的更新:$$f_m(x) = f_{m-1}(x)+\gamma_m G_m(x)$$</li>
</ol>
</li>
<li>输出最终的模型$f_M(x)$</li>
</ol>
<p>下面是来自[2]中的对于不同损失函数下不同残差的计算<br><img src="/img/From-Gradient-Boosting-to-GBT/loss_functions.png" width="500px"><br>可以发现当损失函数为最小平方差时残差就是真实值与预测值的差值</p>
<h2 id="Gradient_tree_boosting">Gradient tree boosting</h2><p>提升树(Gradient tree boosting)故名思议就是使用决策树(一般使用<code>CART</code>树)来作为弱分类器.<br>提升树在第$m$步迭代时将会使用决策树$G_m(x)$来集合残差，现在假设这棵树有$J$个叶子节点，则决策树将会将空间划分为$J$个不相交的区域$R_{1m},R_{2m}…R_{3m}$，以及每个区域都是预测一个常量值，则树模型$G_m(x)$对于特征$x$的输入将可以写成$$G_m(x)=\sum_{j=1}^J b_{jm}I(x \in R_{jm})$$，其中$b_{jm}$表示每个区域的预测值，上面的介绍可以用下图来表示:</p>
<center><img src="/img/From-Gradient-Boosting-to-GBT/cart.png" width="400px"></center>

<p>在实际使用时，$b_{jm}$也会与一个乘子$\gamma_m$相乘,最终模型的训练与上面一小节的介绍一致:<br>$$<br>f_m(x)=f_{m-1}(x)+\gamma_mG_m(x) \\<br>\gamma_m = \underset{\gamma}{argmin} \sum_{i=1}^n L \left(y_i,f_{m-1}(x_i)+\gamma_m G_m(x_i) \right)<br>$$</p>
<blockquote>
<p>可以发现树的模型是关键，一般来时$4 \leq J \leq 8$比较合适，有时候$J=2$就足够了，并且$j &gt; 10$比较少用</p>
</blockquote>
<h2 id="正则化">正则化</h2><blockquote>
<p>其实用过<code>Gradient Boosting</code>的同学应该有同感，<code>Gradient Boosting</code>类型的模型(<code>GBDT</code>)调参很重要-_-，大致可以发现这些参数就是正则化的关键</p>
</blockquote>
<h3 id="调整树的个数">调整树的个数</h3><p>树的个数$M$越多，过拟合的情况可能越为严重，这里树的个数一般使用交叉验证的误差来调整确定</p>
<h3 id="Shrinkage">Shrinkage</h3><p><code>Shrinkage</code>又称学习率，是指在<code>Gradient Boosting</code>训练时不训练全部的残差，而是:$$f_m(x)=f_{m-1}(x)+v \cdot \gamma_mG_m(x) \quad 0 &lt; v \leq 1$$<br>经验表明较小的学习率($v &lt; 0.1$)将会取得较为明显的正则化效果，但是学习率太小会导致训练次数增加..</p>
<blockquote>
<p>感觉这个大致可以这么理解，如果$v=1$，弱分类器犯错一次真的就错了，但是如果$v &lt; 1$时，如果某个分类器犯错了，其他的的弱分类器可能还可以补救^_^</p>
</blockquote>
<h3 id="Stochastic_gradient_boosting">Stochastic gradient boosting</h3><p>随机梯度提升法，表示每一轮迭代时并不是拿所有的数据进行训练，所以按无放回的随机取一定的比率$\eta$进行训练，这里的$ 0.5 &lt; \eta &lt; 0.8$将会取得较为不错的正则化效果，同时随机取样本进行训练还能加快模型的训练速度，并且每次迭代中未被抽中的样本还可以作为(out of bag)[<a href="https://en.wikipedia.org/wiki/Out-of-bag_error]进行估计" target="_blank" rel="external">https://en.wikipedia.org/wiki/Out-of-bag_error]进行估计</a></p>
<h3 id="叶子节点的数量">叶子节点的数量</h3><p>一般这个叶子节点的数量不宜太多（其实可以理解为节点数越多，模型复杂度越高…）</p>
<h3 id="使用惩罚项">使用惩罚项</h3><p>额~貌似<code>L2</code>之类的惩罚项也是可以被加入进去</p>
<h2 id="总结">总结</h2><p><code>Gradient Boosting</code>是非常金典而又重要的提升方法，他与<a href="http://kubicode.me/2016/04/18/Machine%20Learning/AdaBoost-Study-Summary/" target="_blank" rel="external">AdaBoost</a>一样都是讲弱分类器合成强分类，但是其大致区别有:</p>
<ol>
<li><code>Gradient Boosting</code>通过残差来变量的改变错误分类的权重,而<code>AdaBoost</code>就真的直接去修改分类错误的训练权重了</li>
<li><code>Gradient Boosting</code>接入的分类器一般完整的决策树居多，但是<code>AdaBoost</code>一般使用二层决策树</li>
</ol>
<p><code>Gradient Boosting</code>中最有代表性的就是<code>GBDT</code>,该模型虽好，可不要贪杯~使用时理解数据以及正确调参才是王道</p>
<h2 id="参考">参考</h2><p>[1]. <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="external">wiki Gradient boosting</a><br>[2]. The Elements of Statistical Learning<br>[3]. 《统计学习方法》.李航.第八章</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大部分参考(译)自wiki[1]<br>如果说<code>Gradient Boosting</code>是一种机器学习算法框架的话，我想<code>GBT(Gradient Boosting Tree)</code>看做它的实现更为合适</p>
</blockquote>
<h2 id="Gradient_Boosting原理">Gradient Boosting原理</h2><blockquote>
<p>与其他<code>Boosting</code>方法一样，<code>Gradient Boosting</code>通过迭代将弱分类器合并成一个强分类器的方法。</p>
</blockquote>
<p>对于标准的$(x_i,y_i)$训练集，<code>Gradient Boosting</code>在迭代到第$m$次时，可能会得到一个分类能力不是很强的$f_m$模型，但是他下次迭代并不改变$f_m$，而是生成一个这样的新模型:$$f_{m+1} = f_m+G_{m+1}$$使得$f_{m+1}$较$f_m$而言拥有更为强大的分类能力，那么问题来了,这个$G_{m+1}$该如何训练呢?<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[解决在使用scikit-learn时出现ValueError: numpy.dtype has the wrong size的错误]]></title>
    <link href="http://kubicode.me/2016/04/22/Python/Solve-numpy-dtype-In-ValueError-when-using-scikit-learn/"/>
    <id>http://kubicode.me/2016/04/22/Python/Solve-numpy-dtype-In-ValueError-when-using-scikit-learn/</id>
    <published>2016-04-22T01:33:55.000Z</published>
    <updated>2016-04-22T01:51:20.000Z</updated>
    <content type="html"><![CDATA[<p>今天在尝试使用scikit-learn的<code>AdaBoost</code>模型时一直报错，</p>
<pre><code>Traceback (most recent call last):
File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/__init__.py"</span>, line <span class="number">57</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.base</span> import clone
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/base.py"</span>, line <span class="number">11</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.utils</span><span class="class">.fixes</span> import signature
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/utils/__init__.py"</span>, line <span class="number">10</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.murmurhash</span> import murmurhash3_32
File <span class="string">"numpy.pxd"</span>, line <span class="number">155</span>, <span class="keyword">in</span> init sklearn<span class="class">.utils</span><span class="class">.murmurhash</span> (sklearn/utils/murmurhash<span class="class">.c</span>:<span class="number">5029</span>)
ValueError: numpy<span class="class">.dtype</span> has the wrong size, try recompiling
</code></pre><p>以为是<code>numpy</code>包的问题:卸载重装之后还是照样有问题-_-<br><a id="more"></a><br>网上给的建议大都是直接卸载再全部重装，将<code>numpy</code>、<code>scipy</code>和<code>scikit-learn</code>全部卸载了，然后</p>
<pre><code>pip <span class="keyword">install</span> -U numpy scipy scikit-learn
</code></pre><p>装起来。结果一样有问题-_-</p>
<p>继续查资料时终于发现有用的方法了:<br><a href="http://scikit-learn-general.narkive.com/kMA6mRCk/valueerror-numpy-dtype-has-the-wrong-size-try-recompiling" target="_blank" rel="external">http://scikit-learn-general.narkive.com/kMA6mRCk/valueerror-numpy-dtype-has-the-wrong-size-try-recompiling</a></p>
<p>就是不用使用<code>pip install scikit-learn</code>安装，卸载之后直接使用git上<code>https://github.com/scikit-learn/scikit-learn</code>的自己安装</p>
<pre><code>git clone http<span class="variable">s:</span>//github.<span class="keyword">com</span>/scikit-learn/scikit-learn
<span class="keyword">make</span>
sudo <span class="keyword">python</span> setup.<span class="keyword">py</span> install
</code></pre><p>ps：这个时候直接安装可能会出</p>
<pre><code><span class="attribute">RuntimeError</span>: <span class="string">Running cythonize failed!</span>
</code></pre><p>Error提示,这时候安装一下<code>cpython</code>即可</p>
<pre><code>pip <span class="keyword">install</span> cython
</code></pre><p>最后全部安装完之后就可以正常使用了^_^</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<p>今天在尝试使用scikit-learn的<code>AdaBoost</code>模型时一直报错，</p>
<pre><code>Traceback (most recent call last):
File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/__init__.py"</span>, line <span class="number">57</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.base</span> import clone
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/base.py"</span>, line <span class="number">11</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.utils</span><span class="class">.fixes</span> import signature
File <span class="string">"/Library/Python/2.7/site-packages/sklearn/utils/__init__.py"</span>, line <span class="number">10</span>, <span class="keyword">in</span> &lt;module&gt;
from <span class="class">.murmurhash</span> import murmurhash3_32
File <span class="string">"numpy.pxd"</span>, line <span class="number">155</span>, <span class="keyword">in</span> init sklearn<span class="class">.utils</span><span class="class">.murmurhash</span> (sklearn/utils/murmurhash<span class="class">.c</span>:<span class="number">5029</span>)
ValueError: numpy<span class="class">.dtype</span> has the wrong size, try recompiling
</code></pre><p>以为是<code>numpy</code>包的问题:卸载重装之后还是照样有问题-_-<br>]]>
    
    </summary>
    
      <category term="Python" scheme="http://kubicode.me/tags/Python/"/>
    
      <category term="Python" scheme="http://kubicode.me/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[AdaBoost学习总结]]></title>
    <link href="http://kubicode.me/2016/04/18/Machine%20Learning/AdaBoost-Study-Summary/"/>
    <id>http://kubicode.me/2016/04/18/Machine Learning/AdaBoost-Study-Summary/</id>
    <published>2016-04-18T12:30:55.000Z</published>
    <updated>2016-04-22T01:49:25.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p><code>三个凑皮匠，顶一个诸葛亮</code>，打一算法:<code>AdaBoost</code><br>本文是自己对<code>AdaBoost</code>的理解，健忘-_-!! 故记录在此.</p>
</blockquote>
<h2 id="简介">简介</h2><p>痛点:大部分强分类器(<code>LR</code>，<code>svm</code>)分类效果还不错，但是可能会遇到过拟合问题，并且训练相对复杂，耗时~<br>另外大部分弱分类器(<code>阈值分类器</code>,<code>单桩决策树(decision stump)</code>等)，他们分类的效果差，可能是极差，只会出现欠拟合，但是他们训练预测快，很快~</p>
<blockquote>
<p><code>天下武功，唯快不破</code>，<code>做减法不易，但是做加法就相对简单了</code>^_^ 这就是<code>提升方法</code>.</p>
</blockquote>
<p><code>提升方法</code>需要解决的两个问题:</p>
<ol>
<li>在每一轮训练时如何改变数据的权值或概率分布?</li>
<li>如何将弱分类器组合成一个强分类器?</li>
</ol>
<p><code>AdaBoost</code>对此进行了很好的解决:</p>
<ol>
<li><code>分而治之</code>:将前一轮分错的样本加大权重，迫使在第二轮中对这些样本尽量分队，同时减少分对样本的权重.</li>
<li><code>加权多数表决</code>:加大错误率小的弱分类器的权重，使其在最终表决中占较大作用，同时减少错误率较大的弱分类器的权重.</li>
</ol>
<a id="more"></a>
<h2 id="前向分步算法">前向分步算法</h2><p>讲<code>AdaBoost</code>之前，就不得不提到<code>前向分步算法</code>,先来看一个加法模型:<br>$$f(x)=\sum_{m=1}^M \beta_m b(x;\gamma_m)$$其中:</p>
<ul>
<li>$b(x;\gamma_m)$表示基函数</li>
<li>$\gamma_m$表示基函数的参数</li>
<li>$\beta_m$表示基函数的系数</li>
<li>最终加权求和之后形成最终的函数(强模型)</li>
</ul>
<p>假设损失函数为$L(y,f(x))$,则在训练$f(x)$时就是优化损失函数到最小化的问题:<br>$$\underset{\beta,\gamma}{min} \sum_{i=1}^N L\left( y_i,\sum_{m=1}^M \beta_m b(x;\gamma_m) \right)$$</p>
<p>如果直接优化这个损失函数无疑是一个相当复杂的问题:里面嵌入有太多了函数了…，而<code>前向分步算法</code>的策略是:</p>
<pre><code>如果从前往后每一步都是学习一个基函数以及系数，令其逐步逼近优化目标函数，那么复杂度就可以大大简化了<span class="attr_selector">[分而治之]</span>
</code></pre><p>因此每一步只需要如下的损失函数即可:<br>$$\underset{\beta,\gamma}{min} \sum_{i=1}^N L \left( y_i,\beta b(x;\gamma) \right)$$</p>
<p>下面就是<code>前向分步算法</code>的具体步骤:<br><strong>Input</strong>:</p>
<ul>
<li>训练数据集$T=\{(x_1,y_1),(x_2,y_2)…(x_N,y_N)\}$，其中$y_i \in \{+1,-1\}$</li>
<li>损失函数:$L(y,f(x))$</li>
<li>基函数数量:$M$</li>
<li>基函数集合$\{b(x;\gamma_m)\}$</li>
</ul>
<p><strong>Output</strong>:</p>
<ul>
<li>加法模型$f(x)$</li>
</ul>
<p><strong>procedure</strong>:</p>
<ol>
<li>初始化$f_0(x)=0$</li>
<li>循环$m \in \{1….M\}$<ol>
<li>最小化基函数损失函数:$$\underset{\beta_m,\gamma_m}{argmin} \sum_{i=1}^N L\left( y_i,\beta_m b(x;\gamma_m) \right)$$</li>
<li>更新加法模型:$$f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)$$</li>
</ol>
</li>
<li>最终得到加法模型:$$f(x)=f_M(x)=\sum_{m=1}^M \beta_m b(x;\gamma_m)$$</li>
</ol>
<p><code>前向分步算法</code>通过分而治之的方式求得了损失函数值最小的<code>加法函数</code></p>
<blockquote>
<p>整个算法的学习过程可以有很大的想象空间^_^</p>
</blockquote>
<h2 id="AdaBoost算法逻辑">AdaBoost算法逻辑</h2><p>上面一小节介绍了<code>前向分步算法</code>，但是留下来三个问题:</p>
<ol>
<li>对其损失函数$L(y,f(x))$有啥要求?</li>
<li>基函数的系数$\beta$又是怎么计算的?</li>
<li>基函数$b(x;\gamma)$如何设计比较合理?</li>
</ol>
<p><code>AdaBoost</code>正是对此进行一一填坑，它使用<code>指数损失函数</code>、<code>根据分类错误类来计算基函数系数</code>和  。。  基函数到没有指定，不过一般使用<code>阈值函数</code>或者<code>单桩决策树</code>来作为基函数</p>
<blockquote>
<p>因此也可认为<code>AdaBoost</code>是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二分类学习方法.</p>
</blockquote>
<p>先来看下<code>AdaBoost</code>的算法逻辑图:</p>
<p><center><img src="/img/AdaBoost-Study-Summary/exec.png" width="400px"></center></p>
<blockquote>
<p>说明:此图来自<code>PRML Fig14.1</code>[2]，本文的数学符号主要采用[1]的风格，因此有:$y_m(x)\rightarrow G_m(x)$、$Y_M(x) \rightarrow G(x)$</p>
</blockquote>
<p>从图中大致可以发现<code>AdaBoost</code>依次训练多个弱分类器，每个分类器训练完成之后产出一个权重给予下个分类器，下个分类器在此权重上继续进行训练，全部训练完成之后根据弱分类器的系数组合成强分类器，也就是最终的分类模型~</p>
<p>下面就是<code>AdaBoost</code>的具体步骤:<br><strong>Input</strong>:</p>
<ul>
<li>训练数据集$T=\{(x_1,y_1),(x_2,y_2)…(x_N,y_N)\}$，其中$y_i \in \{+1,-1\}$</li>
<li>弱分类器数量:$M$</li>
<li>弱分类器集合$\{G_m(x)\}$</li>
</ul>
<p><strong>Output</strong>:</p>
<ul>
<li>最终模型$G(x)$  （注意：<code>产出的最终模型并不直接是加法模型哦</code>~）</li>
</ul>
<p><strong>procedure</strong>:</p>
<ol>
<li>初始化训练权值的分布:$$D_1=(w_{1,1},\cdot \cdot \cdot , w_{1,i},\cdot \cdot \cdot ,w_{1,N}),w_{1,i}=\frac{1}{N}$$<blockquote>
<p>ps.其实初始化还有其他方式的 such as:初始化为$\frac{0.5}{N_+}$和$\frac{0.5}{N_-}$，其中$N_+$和$N_-$分别代表正负样本的数量，有$N=N_++N_-$</p>
</blockquote>
</li>
<li>循环$m \in \{1….M\}$<ol>
<li>使用带权重分布$D_m$的训练集进行训练，得到基本的弱分类器:$G_m(x)$</li>
<li>计算$G_m(x)$在训练数据集上的分类误差率:$$e_m=P(G_x(x) \neq y_i) = \sum_{i=1}^N w_{m,i} I(G_m(x_i) \neq y_i)$$其中$$ I(G_m(x_i) \neq y_i)=\left\{<br>\begin{aligned}<br>0 &amp; \quad if \quad G_m(x_i) = y_i \\<br>1 &amp; \quad if \quad G_m(x_i) \neq y_i\\<br>\end{aligned}<br>\right.$$<blockquote>
<p>其实就是<code>错误率</code>:分错样本数量占总样本量的比例.</p>
</blockquote>
</li>
<li>计算$G_m(x)$的系数:$$\alpha_m = \frac{1}{2} ln \frac{1-e_m}{e_m}$$</li>
<li><strong>更新训练数据集的权值分布(这点是核心)</strong>:$D_{m+1}=(w_{m+1,1},\cdot \cdot \cdot , w_{m+1,i},\cdot \cdot \cdot ,w_{m+1,N})$<br>其中:$$w_{m+1,i} = \frac{w_{m,i}}{Z_m} e^{-\alpha_m y_i G_m(x_i)}$$ 这里$Z_m$为规范化因子:$$Z_m = \sum_{i=1}^{N} w_{m,i} e^{-\alpha_m y_i G_m(x_i)}$$</li>
</ol>
</li>
<li>构建加法模型:$$f(x)=\sum_{m=1}^M \alpha_m G_m(x)$$产出最终的分类器:$$G(x)=sign(f(x))=sign\left( \sum_{m=1}^M \alpha_m G_m(x) \right)$$其中:$$sign(f(x))=\left\{<br>\begin{aligned}<br>-1 &amp; \quad if \quad f(x) &lt; 0 \\<br>1 &amp; \quad if \quad f(x) \geq 0\\<br>\end{aligned}<br>\right.$$</li>
</ol>
<p>对算法过程中几个重要的点进行一个简单的解释:</p>
<ul>
<li><p>分类器误差率$e_m$对其权重$\alpha_m$的影响:<br><center><img src="/img/AdaBoost-Study-Summary/alpha.png" width="400px"></center><br>从图中可以发现:</p>
<ol>
<li>$e_m$为0.5时其权重$\alpha_m$为0，表示此分类器在最终模型中不起任何作用</li>
<li>$e_m &lt; 0.5$时其$\alpha_m &gt; 0$，表示对最终模型起正向作用,$e_m$的值越小，起到的作用越大</li>
<li>$e_m &gt; 0.5$时其$\alpha_m &lt; 0$，表示对最终模型起父向作用，$e_m$的值越大,起到的负作用也越大</li>
<li>$e_m$不会出现等于0 的情况，因为到了0的时候弱分类器已经全部分正确，也不需要继续更新权重再次训练了</li>
<li>$e_m$也不会出现等于1的情况，因为1表示弱分类器全错，除了程序出问题，应该任何一个弱分类器不会训练到全错的情况吧^_^</li>
</ol>
</li>
<li><p>数据权重的更新策略:<br>原始的更新策略是这样的:$$w_{m+1,i} = \frac{w_{m,i}}{Z_m} e^{-\alpha_m y_i G_m(x_i)}$$原始标签$y_i$和弱分类器结果的输出$G_m(x)$都是$\{+1,-1\}$二值，因此可以将上述更新方式写成:$$w_{m+1,i}=  \left\{<br>\begin{aligned}<br>\frac{w_{m,i}}{Z_m} \times e^{\alpha_m} &amp; \quad if \quad y_i \neq G_m(x)\\<br>\frac{w_{m,i}}{Z_m} \times \frac{1}{e^{\alpha_m}} &amp; \quad if \quad y_i = G_m(x)\\<br>\end{aligned}<br>\right.$$</p>
<blockquote>
<p>观察$w_{m+1,i}$更新时三个元素均恒大于0（$w_{m,i}$可以从$w_1$开始推），因此$w_{m+1,i}$也是恒大于0 ，并且$\sum_{i=1}^N w_{m,i}=1$<br>我们将分类错误率小于0.5的弱分类器称为好分类器，其系数$\alpha_{good}&gt;0$，同时将分类错误率小于0.5的弱分类器称为坏分类器，则其系数$\alpha_{had}&lt;0$，则再观察变形了的权重更新:</p>
</blockquote>
</li>
</ul>
<ol>
<li>如果当前分类器是好分类器，样本$(x_i,y_i)$被错误分类时,$e^{\alpha_m} &gt; 1$,其$w_{m+1,i}$将会被放大，反之正确分类样本的权值将会被缩小</li>
<li>如果当前分类器为坏分类器，样本$(x_i,y_i)$被错误分类时,$e^{\alpha_m} &lt; 1$,其$w_{m+1,i}$将会被缩小，反之正确分类样本的权值将会被放大<blockquote>
<p>这其实应该与误分类样本的权值被放大$e^{2\alpha_m}=\frac{e_m}{1-e_m}$倍一个道理，因为$\frac{e_m}{1-e_m}$不一定恒大于1啊~^_^</p>
</blockquote>
</li>
</ol>
<ul>
<li>弱分类器的权重系数$\alpha_m$:<br>这个权重系数表示了弱分类器$G_m(x)$的重要性，但是$a_m$之和并不为1，另外$G_m(x)$输出的是-1或者1的分类，所以最终模型可以看做一个加权的投票系统^_^</li>
</ul>
<h2 id="AdaBoost-最小化指数误差">AdaBoost-最小化指数误差</h2><p>假如<code>AdaBoost</code>使用的是指数损失函数，则其损失函数为:$$L(y,f(x))=\sum_{i=1}^N e^{-y_if(x)}$$为了优化其损失函数，<code>AdaBoost</code>采用了前向分步算法进行逐步优化,第<code>m</code>轮的迭代需要得到的$\alpha_m$,$G_m$和$f_m(x)$，其中有$$f_m(x)=f_{m-1}(x)+\alpha_m G_m(x)$$，假设前面的$f_{m-1}(x)$已经为最优，则当前需要优化的是:<br>$$L(y,f_m(x))=\sum_{i=1}^N e^{-y_i(f_{m-1}(x)+\alpha_m G_m(x))}$$因为有$\bar{w}_{m,i}=e^{-y_if(x)}$（关于这个式子的成立并不是很理解-_-）所以上述优化目标可以写为$$L(y,f_m(x))= \sum_{i=1}^N \bar{w}_{m,i} e^{-y_i\alpha_m G_m(x)}$$<br>因为我们求的是关于$\alpha_m$和$G_m(x)$的最优化，所以$\bar{w}_{m,i}$相对来说就是常量了.现在假设第$m$轮迭代中有$T_m$个样本分类正确，有$M_m$个样本分类错误，则其优化目标又可以写为:<br>$$\begin{equation}\begin{split} L(y,f_m(x))&amp;=e^{-\alpha_m} \sum_{n \in T_m} w_{m,n}+e^{\alpha_m} \sum_{n \in M_m} w_{m,n}\\<br>&amp;= \left( e^{-\alpha_m} \sum_{n \in T_m} w_{m,n} + e^{-\alpha_m} \sum_{n \in M_m} w_{m,n} \right)+ \left( e^{\alpha_m} \sum_{n \in M_m} w_{m,n} - e^{-\alpha_m} \sum_{n \in M_m} w_{m,n} \right) \\<br>&amp;= e^{-\alpha_m} \sum_{i=1}^N w_{m,i} + (e^{\alpha_m}-e^{-\alpha_m}) \sum_{n=1}^N w_{m,i}I(y_i \neq G_m(x_i))<br>\end{split}\end{equation}$$</p>
<p>接下来惯例的方法就是对$L(y,f_m(x))$求$\alpha$和$G_m(x)$的偏导数，其实在求$G_m(x)$最优时可以发现可以发现第一项和第二项前面的系数并不影响最优化，所以需要求的就是上面步骤中<code>误差率</code>的最优化:$$e_m=P(G_x(x) \neq y_i) = \sum_{i=1}^N w_{m,i} I(G_m(x_i) \neq y_i)$$<br>而得到了最优的$G_m(x)$之后代入$L(y,f_m(x))$求偏导又可以得到最小的$\alpha_m$为:$$\alpha_m = \frac{1}{2} ln \frac{1-e_m}{e_m}$$<br>又是一面熟悉的场景^_^<br>所以可以说<code>AdaBoost</code>整个过程是一直在优化最新函数的指数误差，只是在实际训练时按<code>前向分步算法</code>只需优化当前的误差率即可.</p>
<h2 id="AdaBoost训练误差分析">AdaBoost训练误差分析</h2><pre><code>这里的边界是我自己的理解，与<span class="attr_selector">[1]</span>稍有区别
</code></pre><p>上一小节我们知道在每一步求$\sum_{i=1}^N w_{m,i} I(G_m(x_i) \neq y_i)$的最小化时，其实是在优化最终模型的损失函数$\sum_{i=1}^N e^{-y_if(x)}$,关于模型最终的损失函数有这样一个界限:<br>$$\frac{1}{N} \sum_{i=1}^N I(f(x) \neq y_i) \leq \frac{1}{N} \sum_{i=1}^N e^{-y_if(x)} =  \prod_m Z_m$$<br>因为$e^{-y_if(x)}$一定不会小于0，并且当$f(x) \neq y_i$时，$e^{-y_if(x)}$恒大于1，因此第一、二项的不等式是成立的。接下来看后面的那个等式：</p>
<blockquote>
<p>这里的推导需要用到$Z_m$的变形:$w_{m,i}e^{-\alpha_my_iG_m(x)} = Z_mw_{m+1,i}$</p>
</blockquote>
<p>$$\begin{equation}\begin{split} \frac{1}{N} \sum_{i=1}^N e^{-y_if(x)} &amp;= \frac{1}{N} \sum_{i=1}^N e^{-\sum_{m=1}^M \alpha_my_iG_m(x_i)} \\<br>&amp;= \sum_{i=1}^N w_{1,i} \prod_{m=1}^M e^{-\alpha_m y_i G_m(x_i)} \\<br>&amp;= Z_1 \sum_{i=1}^N w_{2,i} \prod_{m=2}^M e^{-\alpha_my_iG_m(x_i)} \\<br>&amp;= Z_1 Z_2 \sum_{i=1}^N w_{3,i} \prod_{m=2}^M e^{-\alpha_my_iG_m(x_i)} \\<br>&amp;… \\<br>&amp;= Z_1 Z_2 … Z_{m-1}\sum_{i=1}^N w_{M,i}  e^{-\alpha_My_iG_M(x_i)} \\<br>&amp;= \prod_{m=1}^M Z_m<br>\end{split}\end{equation}$$</p>
<p>因此可以发现最优化(最小化)损失函数可以降低最终模型的错误率，同时其错误率与$Z_m$也是有关系的.</p>
<h2 id="AdaBoost黑科技">AdaBoost黑科技</h2><h3 id="Real_AdaBoost">Real AdaBoost</h3><p>上面的<code>AdaBoost</code>的介绍中可以发现$G_m(x)$返回的是-1 或者1 （也就是直接离散值），其实这种方式的返回始终会有一定的<code>gap</code>，那么假如$G_m(x)$输出的是$p(x)=P(y=1|x)$,也就是$x$特征下输出值为1的概率.此时我们最优化的函数为:$$e^{-y \left(f_{m-1}(x)+G_m(p(x) \right)}$$而$G_m(x)=\frac{1}{2} ln \frac{x}{1-x}$<br>这种方式将会修复一定的<code>gap</code>，并且在某些实验中效果也是要好于直接离散的<code>AdaBoost</code></p>
<h3 id="提前终止">提前终止</h3><p>一般情况下是弱分类器训练$M$个才停止，而提前终止只是在训练多个层之后组成的最终分类器的结果已经小于一个置信度的误差，有一种方法可以加快这种判断:</p>
<ol>
<li>一般训练数据里面负样本会远多于正样本</li>
<li>在多个级联的弱分类器在被训练之后，如果正样本被误分为负样本了，则人工将这样正样本进行标记并去去除（质量差的）</li>
<li>那么如果每一轮都是有50%的负样本被监测去重，那么久可以大大减小计算量</li>
<li>最终看假阳性和假阴性来判断是否终止</li>
</ol>
<blockquote>
<p>这种方式靠谱吗？要不就是我理解/翻译错了</p>
</blockquote>
<p>这种提前终止的方法可以降低过拟合的可能性^_^</p>
<h3 id="剪枝">剪枝</h3><p>剪枝指的是去除性能较差的弱分类器，提升效率。最简单的方法是:每个弱分类器都自己的系数和测试误差率极其分布，<code>margineantu</code>则提出的建议为:</p>
<ol>
<li>弱分类器的选择应该分类多样性</li>
<li>如果两个弱分类器很相似，则可以将其中一个给去掉,同时增加相似弱分类器的系数(这里其实就是相当于剪枝了)</li>
</ol>
<h2 id="总结">总结</h2><p><code>AdaBoost</code>秉承<code>三个凑皮匠，顶个诸葛亮</code>的原则，与其说<code>AdaBoost</code>是一个机器学习算法，我更觉得他应该是一个经典的机器学习框架，其优点有:</p>
<ol>
<li>可以较为方便的控制过拟合(不能说避免过拟合，在弱分类器很强的情况下还真会过拟合的吧？看$GBRT$)</li>
<li>有非常强的自适应性</li>
<li>如果弱分类器很简单，则训练预测速度将会很快，毕竟最终复杂度是在弱分类器上乘以$M$</li>
<li>分类效果好，实现简单</li>
<li>可扩展性强~弱分类器可以随意换，甚至损失函数也可以换(比如换最小平方误差)</li>
</ol>
<p>当然也有缺点:</p>
<ol>
<li>相邻两个分类器训练有依赖关系，所以在并行实现下还是需要精心设计。</li>
</ol>
<h2 id="参考:">参考:</h2><p>[1] 《统计学习方法》.李航.第八章<br>[2] 《Pattern Recognition And Machine Learning》.Christopher Bishop.chapter 14<br>[3]  <a href="https://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="external">https://en.wikipedia.org/wiki/AdaBoost</a></p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p><code>三个凑皮匠，顶一个诸葛亮</code>，打一算法:<code>AdaBoost</code><br>本文是自己对<code>AdaBoost</code>的理解，健忘-_-!! 故记录在此.</p>
</blockquote>
<h2 id="简介">简介</h2><p>痛点:大部分强分类器(<code>LR</code>，<code>svm</code>)分类效果还不错，但是可能会遇到过拟合问题，并且训练相对复杂，耗时~<br>另外大部分弱分类器(<code>阈值分类器</code>,<code>单桩决策树(decision stump)</code>等)，他们分类的效果差，可能是极差，只会出现欠拟合，但是他们训练预测快，很快~</p>
<blockquote>
<p><code>天下武功，唯快不破</code>，<code>做减法不易，但是做加法就相对简单了</code>^_^ 这就是<code>提升方法</code>.</p>
</blockquote>
<p><code>提升方法</code>需要解决的两个问题:</p>
<ol>
<li>在每一轮训练时如何改变数据的权值或概率分布?</li>
<li>如何将弱分类器组合成一个强分类器?</li>
</ol>
<p><code>AdaBoost</code>对此进行了很好的解决:</p>
<ol>
<li><code>分而治之</code>:将前一轮分错的样本加大权重，迫使在第二轮中对这些样本尽量分队，同时减少分对样本的权重.</li>
<li><code>加权多数表决</code>:加大错误率小的弱分类器的权重，使其在最终表决中占较大作用，同时减少错误率较大的弱分类器的权重.</li>
</ol>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[聊聊机器学习中的损失函数]]></title>
    <link href="http://kubicode.me/2016/04/11/Machine%20Learning/Say-About-Loss-Function/"/>
    <id>http://kubicode.me/2016/04/11/Machine Learning/Say-About-Loss-Function/</id>
    <published>2016-04-11T13:06:23.000Z</published>
    <updated>2016-04-17T12:25:41.000Z</updated>
    <content type="html"><![CDATA[<p>机器学习算法一般都是对损失函数(<code>Loss Function</code>)求最优，大部分损失函数都是包含两项：<code>损失误差项(loss term)</code>以及<code>正则项(regularization term)</code>:<br>$$J(w)=\sum_iL(m_i(w))+\lambda R(w)$$</p>
<h2 id="损失误差项">损失误差项</h2><p>常用的损失误差项有5种:</p>
<ol>
<li><code>Gold Standard</code></li>
<li><code>Hinge</code>:Svm</li>
<li><code>log</code>:logistic regression(cross entropy error)</li>
<li><code>squared</code>:linear regression</li>
<li><code>Exponential</code>:Boosting</li>
</ol>
<a id="more"></a>
<h3 id="Gold_Standard_Loss">Gold Standard Loss</h3><p><code>Gold Standard</code>又称<code>0-1</code>误差，其结果又称为<code>犯错</code>与<code>不犯错</code>,用途比较广(比如<a href="http://kubicode.me/2015/08/06/Machine%20Learning/Perceptron-Learning-Algorithm/" target="_blank" rel="external">PLA</a>模型)，其损失函数也是相当的简单:<br>$$ y=\left\{<br>\begin{aligned}<br>0 &amp; \quad if \quad m \geq 0 \\<br>1 &amp; \quad if \quad m \le 0\\<br>\end{aligned}<br>\right.$$</p>
<h3 id="Hinge_Loss">Hinge Loss</h3><p><code>Hinge</code>的叫法来源于其损失函数的图形，为一个折线，通用函数方式为:<br>$$L(m_i) = max(0,1-m_i(w))$$</p>
<p><code>Hinge</code>可以解 间距最大化 问题，带有代表性的就是<code>svm</code>,最初的<code>svm</code>优化函数如下:<br>$$\underset{w,\zeta}{argmin} \frac{1}{2}||w||^2+ C\sum_i \zeta_i \\<br>st.\quad \forall y_iw^Tx_i \geq 1- \zeta_i \\<br>\zeta_i \geq 0 $$</p>
<p>将约束项进行变形则为:<br>$$\zeta_i \geq 1-y_iw^Tx_i$$<br>则可以将损失函数进一步写为:<br>$$\begin{equation}\begin{split}J(w)&amp;=\frac{1}{2}||w||^2 + C\sum_i max(0,1-y_iw^Tx_i) \\<br>&amp;= \frac{1}{2}||w||^2 + C\sum_i max(0,1-m_i(w)) \\<br>&amp;= \frac{1}{2}||w||^2 + C\sum_i L_{Linge}(m_i)<br>\end{split}\end{equation}$$</p>
<p>因此<code>svm</code>的损失函数可以看成<code>L2-Norm</code>和<code>Hinge</code>损失误差之和.</p>
<h3 id="Log_Loss">Log Loss</h3><p><code>log</code>类型损失函数的优势可以将连乘转为求和，由于是单调函数，不会改变原结果，并且还很方面求最优，因此<code>log</code>类型的损失函数函数也非常常用，比较著名的一种就是交叉熵(<code>cross entropy</code>)，也就是<code>logistic regression</code>用的损失函数:<br>$$J(w)=\lambda||w||^2+\sum_i y_i log g_w(x_i)+(1-y_i)(log 1-g_w(x_i),y_i \in\{0,1\}$$<br>其中:<br>$$g_w(x_i)=\frac{1}{1+e^{-f_w(x_i)}} \\<br>f_w(x_i) = w^Tx_i<br>$$</p>
<h3 id="Squared_Loss">Squared Loss</h3><p>平方误差，线性回归中最常用:<br>$$L_2(m)=(f_w(x)-y)^2=(m-1)^2$$</p>
<h3 id="Exponential_Loss">Exponential Loss</h3><p>指数误差，在<code>boosting</code>算法中比较常见:<br>$$J(w)=\lambda R(w)+\sum_i exp(-y_if_w(x_i)) \\<br>L_{exp}(m_i) = exp(-m_i(w))<br>$$</p>
<h3 id="误差项对比">误差项对比</h3><p>上面5种误差项的函数为:</p>
<center><img src="/img/Say-About-Loss-Function/error_function.png" width="400px"></center>

<blockquote>
<p>黑色为<code>Squared Loss</code>,<span style="color:red">红色</span>为<code>Hinge Loss</code>,<span style="color:yellow">黄色</span>为:<code>Log Loss</code>,<span style="color:green">绿色</span>为:<code>Exponential Loss</code>,<span style="color:blue">蓝色</span>为:<code>Gold Standard</code></p>
</blockquote>
<p>观察图中:</p>
<ol>
<li><code>Hinge Loss</code>中当$m_i(w) &gt; 1$ 时，其损失项始终未0，当$m_i(w) &lt; 1$时，其损失项的值呈线性增长（正好符合<code>svm</code>的需求）.</li>
<li><code>Squared、Log、Exponential</code>三种损失函数已经<code>Hinge</code>的左侧都是凸函数，并且<code>Gold Stantard</code>损失为他们的下界:<br> $$\zeta_{01} \leq  \hat{\zeta}_{01}(h)+fudge$$</li>
<li>当需要求最大似然时(也就是概率最大化)，使用<code>Log Loss</code>最合适，但是一般会加上一个负号将其转换为求最小</li>
<li>损失函数和的<code>凸特征</code>以及有<code>界</code>是非常重要的，可以防止在一些可以求得无穷的工作上白白浪费时间。有时候为了让函数有界和凸特征，一般会使用一些代理函数来进行替换。</li>
</ol>
<h2 id="正则项">正则项</h2><blockquote>
<p>加入正在项是为了降低模型复杂度，在一定程度上可以有效防止模型过拟合</p>
</blockquote>
<p>常用的正则项有:<br>$$<br>R_2 = \frac{1}{2}||w||^2 \\<br>R_1 = \sum_i |w_i| \\<br>R_0 = |\{i:w_i \neq 0 \}|<br>$$<br>这些正则项可以通用的写成:<br>$$R_p =(\sum_i |w_i|^p)^{\frac{1}{p}}$$</p>
<p>其中：</p>
<ol>
<li>$R_2$最常用，因为它是凸函数，非常方便可以用<code>梯度下降法</code>最优化</li>
<li>$R_1$含有特征选择功能，因此经过$R_1$计算之后会有大量的0权重出现，这样的话我们在实际计算中只需要计算有值特征即可，可以加快速算法的运行速度</li>
<li>$R_0$，额~这个暂时不知道哪里用-_-</li>
</ol>
<p>当$p \leqslant 1$时其正则项就为非凸函数了</p>
<center><img src="/img/Say-About-Loss-Function/reg.png" width="600px"></center>


<h2 id="参考">参考</h2><ol>
<li><a href="http://www.ics.uci.edu/~dramanan/teaching/ics273a_winter08/lectures/lecture14.pdf" target="_blank" rel="external">http://www.ics.uci.edu/~dramanan/teaching/ics273a_winter08/lectures/lecture14.pdf</a></li>
<li><a href="https://www.wikiwand.com/en/Hinge_loss" target="_blank" rel="external">https://www.wikiwand.com/en/Hinge_loss</a></li>
<li><a href="http://www.cnblogs.com/rocketfan/p/4081585.html" target="_blank" rel="external">http://www.cnblogs.com/rocketfan/p/4081585.html</a></li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<p>机器学习算法一般都是对损失函数(<code>Loss Function</code>)求最优，大部分损失函数都是包含两项：<code>损失误差项(loss term)</code>以及<code>正则项(regularization term)</code>:<br>$$J(w)=\sum_iL(m_i(w))+\lambda R(w)$$</p>
<h2 id="损失误差项">损失误差项</h2><p>常用的损失误差项有5种:</p>
<ol>
<li><code>Gold Standard</code></li>
<li><code>Hinge</code>:Svm</li>
<li><code>log</code>:logistic regression(cross entropy error)</li>
<li><code>squared</code>:linear regression</li>
<li><code>Exponential</code>:Boosting</li>
</ol>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Learning To Rank中Pairwise方法的学习]]></title>
    <link href="http://kubicode.me/2016/04/10/Machine%20Learning/LTR-Pairwise-Study/"/>
    <id>http://kubicode.me/2016/04/10/Machine Learning/LTR-Pairwise-Study/</id>
    <published>2016-04-10T09:05:18.000Z</published>
    <updated>2016-05-08T15:28:16.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>由于<code>Pairwise</code>方式的排序学习方法 训练样本构建方便、速度快同时效果也还可以，因此在工业界和学术的应用非常广泛^_^</p>
</blockquote>
<h2 id="2002-RankSvm">2002-RankSvm</h2><p><code>RankSvm</code>是最经典的一种，将其余的相关实现方法学习总结简单的记录到本文中。<br><code>RankSvm</code>的学习记录在<a href="http://kubicode.me/2016/03/30/Machine%20Learning/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/" target="_blank" rel="external">这里</a></p>
<h2 id="2006-IRSVM">2006-IRSVM</h2><p><code>IRSVM</code>直接是<code>RankSvm</code>的改进，<code>RankSvm</code>的训练目标是让序列pair的<code>不一致pair对</code>对最少，其优化函数为：<br>$$\tau(r_a,r_b)=\frac{P-Q}{P+Q}=1-\frac{2Q}{\binom{m}{2}}$$<br>因此直接暴露了两大问题:</p>
<h3 id="问题1:位置误差">问题1:位置误差</h3><a id="more"></a>
<pre><code><span class="tag">Example1</span>:
档位<span class="pseudo">:3</span>,2,1
排序1<span class="pseudo">:2</span> 3 2 1 1 1 1
排序2<span class="pseudo">:3</span> 2 1 2 1 1 1
</code></pre><p>从样例1中可以看到如果是按$\tau$最大化进行优化的话，<code>排序1</code>中<code>2 3</code>为不一致pair,排序2中<code>1 2</code>为不一致pair，因此他们的$\tau$得分是一致的，但是明显可以看到排序2的应该为更优，因为越<code>top</code>级别的重要性越大<br>因此<code>IRSVM</code>考虑了计算$\tau$时将位置顺序纳入误差</p>
<h3 id="问题2:长度误差">问题2:长度误差</h3><pre><code><span class="tag">Example2</span>:
档位<span class="pseudo">:3</span>,2,1
排序3<span class="pseudo">:3</span> 2 2 1 1 1 1
排序4<span class="pseudo">:3</span> 3 2 2 2 1 1 1 1 1
</code></pre><p>现观察样例2，可以发现<code>排序3</code>和<code>排序4</code>中均未出现<code>不一致pair</code>的文档对，因此他们的$\tau$得分是一样的，并且均为1，但是<code>排序3</code>中存在<code>28</code>个文档对，而<code>排序4</code>中存在<code>45</code>个文档对，所以<code>排序4</code>存在更多的训练数据，因此<code>排序4</code>的数据相当于<code>RankSvm</code>来说更加重要。<br>因此<code>IRSVM</code>将召回的文档个数纳入了排序</p>
<blockquote>
<p>其实这点比较纠结，实际使用中会进行数据过滤，而且最终训练的时候也一般都是取<code>top10</code>进行训练，所以这个问题并不会很明显</p>
</blockquote>
<h3 id="优化学习方法">优化学习方法</h3><p>所以<code>IRSVM</code>考虑了不同排序位置的不同重要性，以及各个<code>query</code>召回的数量，对原始的<code>RankSVM</code>损失函数进行修改得到如下:<br>$$\underset{w}{min} \sum_{i=1}^N \tau_{k(i)} \mu_{q(i))} [1-y_i(w,x_i^{(1)}-x_i^{(2)})]_+ + \lambda||w||^2$$</p>
<p>其实重要是添加了位置得分因子$\tau_{k(i)}$，使用<code>NDCG@1</code>中的方法进行折损，以及添加了<code>query</code>召回的文档长度因子$\mu_{q(i)}$，使用的是最简单粗暴的$\frac{1}{n_q}$进行计算，其中$n_q$表示召回的文档数量。</p>
<p>[2]中提出<code>IRSVM</code>的时候还使用了<code>SGD</code>和<code>线性规划</code>进行了优化</p>
<h2 id="2007-GBRank">2007-GBRank</h2><p><code>GBRank</code>的相关学习记录<a href="http://kubicode.me/2016/05/08/Machine%20Learning/GBRank-A-PairWsie-LTR-Base-on-Regression-Framework/" target="_blank" rel="external">在这里</a></p>
<h2 id="参考">参考</h2><ol>
<li>《Learning to Rank for Information Retrieval and Natural Language Processing》.Hang Li</li>
<li>Cao Y, Xu J, Liu T Y, et al. Adapting ranking SVM to document retrieval[C]// SIGIR 2006: Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, Usa, August. 2006:186-193.</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>由于<code>Pairwise</code>方式的排序学习方法 训练样本构建方便、速度快同时效果也还可以，因此在工业界和学术的应用非常广泛^_^</p>
</blockquote>
<h2 id="2002-RankSvm">2002-RankSvm</h2><p><code>RankSvm</code>是最经典的一种，将其余的相关实现方法学习总结简单的记录到本文中。<br><code>RankSvm</code>的学习记录在<a href="http://kubicode.me/2016/03/30/Machine%20Learning/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/">这里</a></p>
<h2 id="2006-IRSVM">2006-IRSVM</h2><p><code>IRSVM</code>直接是<code>RankSvm</code>的改进，<code>RankSvm</code>的训练目标是让序列pair的<code>不一致pair对</code>对最少，其优化函数为：<br>$$\tau(r_a,r_b)=\frac{P-Q}{P+Q}=1-\frac{2Q}{\binom{m}{2}}$$<br>因此直接暴露了两大问题:</p>
<h3 id="问题1:位置误差">问题1:位置误差</h3>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RankSvm-基于点击数据的搜索排序算法]]></title>
    <link href="http://kubicode.me/2016/03/30/Machine%20Learning/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/"/>
    <id>http://kubicode.me/2016/03/30/Machine Learning/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/</id>
    <published>2016-03-30T12:49:53.000Z</published>
    <updated>2016-04-04T15:21:20.000Z</updated>
    <content type="html"><![CDATA[<pre><code>RankSvm是Pairwise的学习排序中最早也是非常著名的一种算法，主要解决了传统PontWise构建训练样本难的问题，
并且基于<span class="built_in">Pair</span>的构建的训练样本也更为接近排序概念
</code></pre><h2 id="基本介绍">基本介绍</h2><p>RankSvm是在2002年提出的，之前工作关于LTR的工作貌似只有Pointwise相关的,比如PRanking,这样的排序学习算法Work需要含有档位标注的训练样本，一般有以下几种获取方式：</p>
<ol>
<li>需要人工/专家标注</li>
<li>诱导用户对展现的搜索结果进行反馈</li>
</ol>
<p>这样就会存在会成本高、可持续性低、受标准者影响大等缺点。<br><a id="more"></a></p>
<p>而RankSvm只需要根据搜索引擎的点击日志构建<code>Pair</code>对即可，相对于先前的工作在算法的实用性上有了非常大的改善。</p>
<h2 id="训练样本设计">训练样本设计</h2><p>一般搜索引擎都会记录用户搜索后展现的结果以及用户的点击情况，这种日志成本较低，并且改造系统也较为方便，而RankSvm的训练样本就是从这种点击日志中进行提取。</p>
<p>由于用户点击的<code>doc</code>概率和排序的位置影响很大，虽然一般都是偏爱相关性较大的<code>doc</code>，但是如果这种<code>doc</code>排在很后面的话其实用户也几乎不会点，所有<code>rankSvm</code>就只考虑top级别的点击日志，一般为<code>top 10</code></p>
<p>另外在构建训练数据时同一<code>query</code>下认为用户被点击<code>doc</code>相关性要高于没有被点击的<code>doc</code>，但是由于用户是从上往下浏览网页的，所以排在前面的<code>doc</code>被点击的概率会大于后面的<code>doc</code>，因此<code>RankSvm</code>使用的最终策略为:</p>
<blockquote>
<p><code>pair</code>构建策略:给定一组排序情况($doc_1,doc_2,doc_3,…$),以及$C$记录了用户点击<code>doc</code>的情况,则有<br>$$doc_i{\overset{r^*}{&lt;}} doc_j$$对于所有<code>pair</code>对$1 \leq i$,同时$i \in C$ ,$j \notin C$</p>
</blockquote>
<p>$r^*$表示应有的优化排序，也就是<code>被点击文档</code>的相关性要大于<code>排在该文档前面</code>并且<code>未被点击的文档</code>，看上去很绕口，看个栗子：</p>
<p>假如某个用户搜索某个<code>query</code>得到首页10个<code>doc</code>，按顺序使用$doc_i$进行表示,如果该用户点击了$1,3,7$三个文档，则认为有:<br>$$doc_3 {\overset{r^*}{&lt;}} doc_2 \\<br>doc_7 {\overset{r^*}{&lt;}} doc_2    \\<br>doc_7 {\overset{r^*}{&lt;}} doc_4    \\<br>doc_7 {\overset{r^*}{&lt;}} doc_5    \\<br>doc_7 {\overset{r^*}{&lt;}} doc_6$$</p>
<p>表示该<code>query</code>下$doc_3$的相关性要高于$doc_2$，理应排在前面，而$doc7$的相关性也应该要高于$doc_{2,4,5,6}$，但是可以发现未见$doc_1$的相关性鉴定，这是由于$doc_1$已经是排在了第一位，本身位置点击概率就就是最高的，所以无法判断与其他文档相关性的高低，同理，$doc_{8,9,10}$也未纳入相关性的排序中。</p>
<p>训练样本可以通过这样方式进行构建，但是遗憾的是并没有一种机器学习算法能直接使用这种数据进行训练学习-_-</p>
<h2 id="基本思想">基本思想</h2><p>在上面的栗子中，我们希望用新算法完成$doc_{1,3,7}$排在<code>top 3</code>，那这样的文档的真实相关性高的将会排到前面，<code>RankSvm</code>采用$Kendall’s \quad \tau$来统计实际排序与算法排序的度量，先看下面两个变量:</p>
<ol>
<li>$P$表示排序序列中保持一致性的<code>Pair</code>对数量，也就是真实相关性高的排在第的前面。</li>
<li>$Q$表示排序序列中保持不一致的<code>Pair</code>对数量（就是为逆序了），也就是由于算法的误差导致真实相关性低的排在了高的前面</li>
<li>同时$P+Q=\binom{m}{2}$，$m$表示序列中文档的数量，因为长度为$m$的序列可能组成的<code>pair</code>对为$m$的2组合</li>
</ol>
<p>则$\tau$的计算方式为:<br>$$\tau(r_a,r_b)=\frac{P-Q}{P+Q}=1-\frac{2Q}{\binom{m}{2}}$$</p>
<blockquote>
<p>$r_a$为真实排序，$r_b$为算法排序</p>
</blockquote>
<p>比如实际文档中的顺序为:<br>$$d_1{\overset{r^*}{&lt;}}d_2{\overset{r^*}{&lt;}}d_3{\overset{r^*}{&lt;}}d_4{\overset{r^*}{&lt;}}d_5$$<br>但是算法的排序顺序为:<br>$$d_3{\overset{\hat{r}}{&lt;}}d_1{\overset{\hat{r}}{&lt;}}d_2{\overset{\hat{r}}{&lt;}}d_4{\overset{\hat{r}}{&lt;}}d_5$$</p>
<p>因此可以发现算法排序中有3对<code>pair</code>不一致了($\{d_2,d_3\}$,$\{d_1,d_2\}$,$\{d_1,d_3\}$)，所以$Q=3$，$Q=7$，最终的$\tau(r,\hat{r})=\frac{7-3}{10}=4$<br>$\tau$的值越大，表示排序效果越接近真实，比如上面的$\tau(r,r)=\frac{10-0}{10}=1$</p>
<blockquote>
<p><code>RankSvm</code>还证明了由$Q$的倒数正相关的一个式子为平均准确率指标的下界(具体证明看原文附录):<br>$$AvgPrec(\hat{r}) \geq \frac{1}{R} \left[Q+\binom{R+1}{2}\right]^{-1} \left(\sum_{i=1}^{R} \sqrt{i}\right)^2$$<br>$R$为排序出现的文档中与query相关文档数量(这个是<a href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/#MAP" target="_blank" rel="external">Mean Average Precision</a>中的标注，与<code>Pair</code>对的样本稍微有点差别)<br>从这个角度也可以看到$\tau$来衡量排序效果好坏的合理性.</p>
</blockquote>
<p>假设现在有$n$个$q_i$作为训练样本，他们各自的目标排序为$r_i^*$，也就是:<br>$$(q_1,r_1^*),(q_2,r_2^*),(q_3,r_3^*),…(q_n,r_n^*)$$</p>
<p>其中算法排序为$\hat{r}_i$，则排序算法的优化目标是将下列式子<br>$$\tau_{s}=\frac{1}{n} \sum_{i=1}^{n}\tau(r_i^*，\hat{t}_i)$$<br>进行最大化.</p>
<h2 id="RankSvm排序">RankSvm排序</h2><p>假设能找到一个排序算法能使得上面的$\tau_s$得到最大化，对于一个指定的查询$q$，每个文档$d_i$使用特征向量映射方法$\Phi(q,d_i)$，如果是直接使用线性排序方法:<br>$$(d_i,d_j) \in \hat{r} \Leftrightarrow \vec{w} \Phi(q,d_i)&gt;\vec{w} \Phi(q,d_j)$$<br>$\vec{w}$表示权重向量，线性排序下，排序分数为<code>权重 x 特征向量</code></p>
<p><center><img src="/img/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/linear_example.png" width="400px"></center><br>上图表示一个二维的<code>权重</code>与<code>特征</code>图，在排序的顺序的就是特征在权重上的映射位置顺序，比如单从$W_1$维度进行观察可以看到的排序顺序为<code>{1,2,3,4}</code>，而如果按$W_2$维度则是<code>{2,3,1,4}</code>。</p>
<p>为了最大化$\tau_s$，可以最小化$    Q$来代替，也就是说对于线性的排序，$Q=0$就是表示下面的等式全成立(也就是最大化)<br>$$<br>\forall (d_i,d_j) \in r_1^* : \vec{w} \Phi(q,d_i)&gt;\vec{w} \Phi(q,d_j) \\<br>… \\<br>\forall (d_i,d_j) \in r_n^* : \vec{w} \Phi(q,d_i)&gt;\vec{w} \Phi(q,d_j)<br>$$</p>
<p>不幸的是，优化这个是一个<code>NP难题</code>。<br>然而<code>SVM</code>在由于软间距最大时可以看到熟悉的身影：<br>minimize:<br>$$\frac{1}{2} \vec{w} \cdot \vec{w} + C \sum \xi_{i,j,k}$$<br>subject to:<br>$$<br>\forall (d_i,d_j) \in r_1^* : \vec{w} \Phi(q,d_i) \geq \vec{w} \Phi(q,d_j) + 1 - \xi_{i,j,1} \\<br>… \\<br>\forall (d_i,d_j) \in r_n^* : \vec{w} \Phi(q,d_i) \geq \vec{w} \Phi(q,d_j) +1 - \xi_{i,j,n} \\<br>\forall_i \forall_j \forall_k:\xi_{i,j,k} \geq 0<br>$$</p>
<blockquote>
<p>$\xi$为松弛项，$C$表示平衡项</p>
</blockquote>
<p>因此优化该问题时可以将约束转为:<br>$$\vec{w} \Phi(q,d_i)-\vec{w} \Phi(q,d_j) \geq 1 - \xi_{i,j,1}$$<br>并且由于是线性排序，可以进一步精简为:<br>$$\vec{w} \left(\Phi(q,d_i)-\Phi(q,d_j)\right) \geq 1 - \xi_{i,j,1}$$</p>
<p>在<code>Pair</code>对中只可能$d_i$是否排在$d_j$前面是一个二值结果，所以我们可以将$d_i$排在$d_j$前面的<code>Pair</code>为正标签，否则为负标签:</p>
<p>$$ y=\left\{<br>\begin{aligned}<br>+1 &amp; \quad if \quad \vec{w} \Phi(q,d_i)&gt;\vec{w} \Phi(q,d_j) \\<br>-1 &amp; \quad otherwise\\<br>\end{aligned}<br>\right.$$</p>
<p>则最终可以将约束可以写成:<br>$$y_i \cdot \vec{w} \left(\Phi(q,d_i)-\Phi(q,d_j)\right) \geq 1 - \xi_{i,j,1}$$</p>
<blockquote>
<p>其中传统的偏置项在<code>RankSvm</code>是不需要的，以为正好<code>Pair</code>相减时就消掉了</p>
</blockquote>
<p>这样就可以完全将上面构建的样本转为一个分类问题，使用<code>SVM</code>的对偶形式进行求解，并且还可以使用核函数进行非线性的分类^_^</p>
<p>训练完<code>Svm</code>之后，在正真排序时只需要将原始$doc$的特征向量输入<code>Svm</code>模型即可:<br>$$resv(q,d_i)= \vec{w} \Phi(q,d_i) = \sum_l^n a_{l}^*y_i(\Phi(q,d_i) \cdot \Phi(q,d_l)) $$</p>
<p><center><img src="/img/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/binary_classifiy.png" width="400px"></center><br>上面是表示两个不同的<code>query</code>所表示的特征空间，不同的形状表示文档与对应<code>query</code>的相关性档位,三角形$x_1$表示相关性档位高，圆圈$x_2$表示相关性档位一般，叉叉$x_3$表示相关性档位差。<br>将这些样本转为<code>Pair</code>对之后可以有：</p>
<p><center><img src="/img/RankSvm-Optimizing-Search-Engines-using-Clickthrough-Data/pair_wise_result.png" width="400px"></center><br>可以发现$x_1-x_3$和$x_1-x_2$为正样本，而$x_2-x_1$和$x_3-x_1$为负样本，因此可以形成对应的训练数据进行训练,其实这样形成的样本是对称的，因此在实际使用中一般只保留一侧即可。</p>
<h2 id="总结">总结</h2><p><code>RankSvm</code>很好的解决原始训练样本构建难的问题，根据点击日志构建样本，既考虑了<code>doc</code>之间的顺序，又保证了可持续性，并且其<code>Pair</code>对的训练正好可以使用<code>Svm</code>进行求最优化，而<code>Svm</code>分类器已经是非常成熟并且广泛使用的一种机器学习算法。<br>因此<code>RankSvm</code>虽然在2002年就提出，但是至今在工业界还是广泛使用，但是现在的主要难点是样本<code>Pair</code>对的构建，针对不同的场景也不同的策略，不一定只是根据点击顺序，实际使用中还要考虑新样本数据。</p>
<h2 id="参考">参考</h2><ol>
<li>Joachims T. Optimizing search engines using clickthrough data[C]// Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002:133-142.</li>
<li>《Learning to Rank for Information Retrieval and Natural Language Processing》.Hang Li</li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>RankSvm是Pairwise的学习排序中最早也是非常著名的一种算法，主要解决了传统PontWise构建训练样本难的问题，
并且基于<span class="built_in">Pair</span>的构建的训练样本也更为接近排序概念
</code></pre><h2 id="基本介绍">基本介绍</h2><p>RankSvm是在2002年提出的，之前工作关于LTR的工作貌似只有Pointwise相关的,比如PRanking,这样的排序学习算法Work需要含有档位标注的训练样本，一般有以下几种获取方式：</p>
<ol>
<li>需要人工/专家标注</li>
<li>诱导用户对展现的搜索结果进行反馈</li>
</ol>
<p>这样就会存在会成本高、可持续性低、受标准者影响大等缺点。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[McRank:一种基于多分类和梯度提升树的排序学习]]></title>
    <link href="http://kubicode.me/2016/03/28/Machine%20Learning/McRank-Learning-to-Rank-Multiple-Classification-and-Gradient-Boosting/"/>
    <id>http://kubicode.me/2016/03/28/Machine Learning/McRank-Learning-to-Rank-Multiple-Classification-and-Gradient-Boosting/</id>
    <published>2016-03-28T08:37:53.000Z</published>
    <updated>2016-03-30T07:45:10.000Z</updated>
    <content type="html"><![CDATA[<pre><code><span class="name">McRank</span>是学习排序(<span class="name">Learning</span> <span class="atom">to</span> <span class="name">Rank</span>)的单文档排序分支(<span class="name">Pointwise</span>)中较为经典的一种，本文是读原<span class="name">Paper</span>[<span class="number">1</span>]之后自己的一个理解.
</code></pre><h2 id="基本介绍">基本介绍</h2><p><code>MacRank</code>的全称是<code>Multiple Classification Rank</code>,可以理解为将<a href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/" target="_blank" rel="external">学习排序</a>转为机器学习中的一个多分类问题.<br><code>McRank</code>对<code>DCG</code>指标进行优化，并且可以证明<code>DCG</code>的误差可以被分类误差给<code>bounded</code>住.</p>
<h2 id="折损累积增益">折损累积增益</h2><blockquote>
<p><code>DCG</code>(Discounted Cumulative Gain)是在信息检索领域评估一个<code>rank</code>好坏的常用指标。(在实际使用中一般会进行归一化，称为<code>NDCG</code>，可以看<a href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/#NDCG" target="_blank" rel="external">这里</a>).</p>
</blockquote>
<p>假设在指定的<code>query</code>下通过某个排序算法对$n$个文档进行排序，则可以得到<br>$$DCG=\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-1)$$<br><a id="more"></a></p>
<p>其中:</p>
<ol>
<li>$i$表示原文档的索引顺序</li>
<li>$c_{[\pi_i]}=log(i+1)$</li>
<li>$y_i$表示对应文档与<code>query</code>相关性的程度，一般用档位$\{0,1,2,3,4\}$来表示</li>
</ol>
<p>最终的<code>DCG</code>值越大，表示排序效果越好,假如直接根据档位降序得到的排序结果中<code>DCG</code>是最大的，实际使用中一般根据当前可能的最大<code>DCG</code>进行归一化</p>
<h2 id="排序思想">排序思想</h2><p>现在已经知道了文档与<code>query</code>之间一般用档位衡量，而假如按档位降序的<code>DCG</code>值最高,所以排序问题可以转为指定<code>query</code>对文档相关性类别的预测，即多分类问题。</p>
<h3 id="DNCG误差计算">DNCG误差计算</h3><p>现在可以这么理解，我们希望的是<code>DCG</code>越大越好，也即是<code>DCG</code>误差越小越好，但如果是分类问题将直接优化的是分类误差，那如果<code>DCG</code>误差能够被分类误差给<code>bouded</code>住，就可以通过优化分类误差来间接的优化<code>DCG</code>误差了.</p>
<p>对于一个排序的置换映射函数$\pi$,<code>DCG</code>误差为$DCG_g-DCG_{\pi}$，其中$DCG_g$表示最优排序,就是根据实际的<code>query-doc</code>相关性分档降序的排序，所以肯定有$DCG_g \geq DCG_{\pi}$</p>
<p>现给定$n$个<code>URLS</code>的顺序为$\{1,2,3…n\}$，假设分类器分配的相关结果为$\hat{y}_i \in \{0,1,2,3,4\}$，置换映射函数$\pi$直接根据相关性进行排序，高档位的排在前面，相同档位可以随意排序，则可以有以下证明:</p>
<blockquote>
<p>先看变量^_^<br>$y_i$表示<code>query-doc</code>的实际相关性<br>$\hat{y}_i$表示<code>query-doc</code>的分类器预测相关性<br>$c_{[g_i]}$表示根据实际相关性得到的排序<br>$c_{[\pi_i]}$表示根据预测相关性得到的排序</p>
</blockquote>
<p>$$\begin{equation}\begin{split}DCG_{\pi}&amp;=\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-1) \\<br>&amp;=\sum_{i=1}^{n}c_{[\pi_i]}(2^{\hat{y}_i}-1)+\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-2^{\hat{y}_i}) \\<br>&amp;\geq \sum_{i=1}^{n}c_{[g_i]}(2^{\hat{y}_i}-1)+\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-2^{\hat{y}_i}) \\<br>&amp;=\sum_{i=1}^{n}c_{[g_i]}(2^{y_i}-1)-\sum_{i=1}^{n}c_{[g_i]}(2^{y_i}-2^{\hat{y}_i})+\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-2^{\hat{y}_i}) \\<br>&amp;=DCG_g+\sum_{i=1}^{n}(c_{[\pi_i]}-c_{[g_i]})(2^{y_i}-2^{\hat{y}_i})<br>\end{split}\end{equation}$$</p>
<p>解释下不等式$\sum_{i=1}^{n}c_{[\pi_i]}(2^{\hat{y}_i}-1) \geq \sum_{i=1}^{n}c_{[g_i]}(2^{\hat{y}_i}-1)$成立的原因:$c_{[\pi_i]}$是根据相关性$\hat{y}_i$排序得到的，也就是上面提到的最优排序，得到的$DCG$值是最大的(当然这个是分类器的预测值，不是真实值，就是假象的意思-_-)，所以换一种顺序$c_{[g_i]}$其$DCG$的值必定会小于等于最大值.</p>
<p>根据上面的推导就可以直接得到$DCG$的误差了:<br>$$\begin{equation}\begin{split}DCG_g-DCG_{\pi} &amp;\leq \sum_{i=1}^{n}(c_{[g_i]}-c_{[\pi_i]})(2^{y_i}-2^{\hat{y}_i}) \\<br>&amp; \leq \left(\sum_{i=1}^{n}(c_{[g_i]}-c_{[\pi_i]})^2\right)^{\frac{1}{2}}\left(\sum_{i=1}^{n}(2^{y_i}-2^{\hat{y}_i})^2\right)^{\frac{1}{2}} \\<br>&amp;\leq \left(2\sum_{i+1}^{n}c_{[i]}^2-2n\prod_{i=1}^nc_{[i]}^{\frac{2}{n}}\right)^{\frac{1}{2}} 15 \left(\sum_{i=1}^{n}1_{y_i \neq \hat{y}_i}\right)^{\frac{1}{2}} \\<br>&amp;= 15\sqrt{2} \left(\sum_{i+1}^{n}c_{[i]}^2-n\prod_{i=1}^nc_{[i]}^{\frac{2}{n}}\right)^{\frac{1}{2}}\left(\sum_{i=1}^{n}1_{y_i \neq \hat{y}_i}\right)^{\frac{1}{2}}<br>\end{split}\end{equation}$$</p>
<p>上面公式<code>1~2</code>是不等式是根据<code>柯西不等式</code>得到的，第<code>2~3行</code>平方展开的不等式成立是因为:</p>
<ol>
<li>$\sum_{i=1}^{n}c_{[\pi_i]}^2=\sum_{i=1}^{n}c_{[g_i]}^2=\sum_{i=1}^{n}c_{[i]}^2$,$\prod_{i=1}^{n}c_{[\pi_i]}^2=\prod_{i=1}^{n}c_{[g_i]}^2=\prod_{i=1}^{n}c_{[i]}^2$，他们虽然是顺序不一样，但是他们集合的内容是一样的，所以<code>求和</code>和<code>连乘</code>的等式是成立的。</li>
<li>两种相关性$y_i$和$\hat{y}_i$的值在<code>0~4</code>范围内,因此有$(2^{y_i}-2^{\hat{y}_i})^2\leq 15$，因为$2^4-2^0=15$</li>
</ol>
<p>因此当需要最小化<code>DCG</code>误差时只需要最小化分类误差$\sum_{i=1}^{n}1_{y_i \neq \hat{y}_i}$即可，但是该误差非凸也非平滑，实际使用中使用代理损失函数进行优化:<br>$$\sum_{i=1}^{N}\sum_{k=0}^{K-1}-log(p_{i,k})1_{y_i=k}$$<br>其中$p_{i,k}$表示<code>doc</code>输入每个档位的概率，$K$是总的档位数。</p>
<blockquote>
<p>我感觉:上面不等式里面的排序顺序时间使用$c_{[i]}$代替了，虽然表面上和分档结果无关，因此只需要优化分档即可，但是。。。实际上$c_{[\pi_i]}$的顺序是和分档预测有关的啊….</p>
</blockquote>
<h3 id="分类排序">分类排序</h3><p>上面提到过有了分档结果之后可以按档位顺序降序排序，得到的<code>DCG</code>就是最优的，但是这里存在一个问题，那就是相同档位之间是可以随便排的，就是导致排序的不稳定性，为了得到一种良好的排序机制，<code>McRank</code>在实际排序中会将分类结果转为一个连续的分数，按这个分数进行排序.</p>
<p>假设训练是$\{y_i,x_i\}_i^N$，$y_i$表示多分类的分档,则最终将会学习到的是每个类别的概率$p_{i,k}=Pr(y_i=k)$，则最后的排序分数为:<br>$$S_i=\sum_{k=0}^{K-1}p_i^kT(k)$$<br>其中$T(k)$表示随档位单调递增的函数，比如$T(k)=k$或者$T(k)=2^k$（<code>McRank</code>用的是前者）</p>
<blockquote>
<p>不同的单调递增函数不会对排序进行影响，同时如果对函数进行线性变换也会改变排序结果</p>
</blockquote>
<p>最终<code>McRank</code>是使用<code>Boosting Tree</code>进行多分类预测..</p>
<h3 id="有序分类">有序分类</h3><blockquote>
<p>好吧，其实上面<code>McRank</code>已经讲完了，但是Paper里面提到有序分类(<code>Ordinal Classfifcation</code>，貌似就是<code>Pointwise</code>里面的第三个分支)可以提升排序结果。</p>
</blockquote>
<p>这里在进行多分类时，可以发现每个类别(档位)并不是平等的，比如4档就是要比0档相关性更好，为了考虑这种的偏移，有序分类就是干这个的，多类别有序分类是学习一段区间内的累积概率$Pr(y_i&lt;k)$</p>
<p>首先将训练数据点分类两种$\{y_i \geq 4 \}$和$\{y_i \leq 3\}$，那这样称为了一个二分类问题，这里一样使用<code>boosting Tree</code>进行分类，同样关于$\{y_i \leq 3\}$也可以分为$\{y_i \geq 3 \}$和$\{y_i \leq 2\}$，如果迭代就可以进行变相多分类.</p>
<p>这种方式就是考虑目标类目的不均等性，带来的问题就是会增加训练开销(因为有重复计算，并且可能会带来较大的样本<a href="http://kubicode.me/2015/08/30/Machine%20Learning/Multiclass-Classification/#One-Vs-All" target="_blank" rel="external">不均衡性</a>。。。。)</p>
<blockquote>
<p>还有一个坏消息。。。<code>McRank</code>的Paper实验里面这种方式并没有比普通多分类提升多少效果-_-</p>
</blockquote>
<h2 id="总结">总结</h2><p><code>McRank</code>是非常经典的一种<code>Pointwise</code>学习排序，将排序转为机器学习的多分类预测，并且对其排序指标<code>DCG</code>误差可以被分类误差给<code>bounded</code>住，最终将分类结果的概率转为一个连续分数进行最终的排序，在实验里面显示该方法比基于回归的<code>subRank</code>以及<code>pairwise</code>的<code>LambdaRank</code>效果更好。</p>
<blockquote>
<p><code>McRank</code>速度快，效果也还行，最大的问题就是<code>训练样本的构建</code>比较麻烦..</p>
</blockquote>
<h2 id="参考">参考</h2><ol>
<li>Li P, Burges C J C, Wu Q. McRank: Learning to Rank Using Multiple Classification and Gradient Boosting[J]. Advances in Neural Information Processing Systems, 2007:897-904.</li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code><span class="name">McRank</span>是学习排序(<span class="name">Learning</span> <span class="atom">to</span> <span class="name">Rank</span>)的单文档排序分支(<span class="name">Pointwise</span>)中较为经典的一种，本文是读原<span class="name">Paper</span>[<span class="number">1</span>]之后自己的一个理解.
</code></pre><h2 id="基本介绍">基本介绍</h2><p><code>MacRank</code>的全称是<code>Multiple Classification Rank</code>,可以理解为将<a href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/">学习排序</a>转为机器学习中的一个多分类问题.<br><code>McRank</code>对<code>DCG</code>指标进行优化，并且可以证明<code>DCG</code>的误差可以被分类误差给<code>bounded</code>住.</p>
<h2 id="折损累积增益">折损累积增益</h2><blockquote>
<p><code>DCG</code>(Discounted Cumulative Gain)是在信息检索领域评估一个<code>rank</code>好坏的常用指标。(在实际使用中一般会进行归一化，称为<code>NDCG</code>，可以看<a href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/#NDCG">这里</a>).</p>
</blockquote>
<p>假设在指定的<code>query</code>下通过某个排序算法对$n$个文档进行排序，则可以得到<br>$$DCG=\sum_{i=1}^{n}c_{[\pi_i]}(2^{y_i}-1)$$<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[从Bayesion的角度来看Logistic Regression]]></title>
    <link href="http://kubicode.me/2016/03/26/Machine%20Learning/Bayesian-Logistic-Regression/"/>
    <id>http://kubicode.me/2016/03/26/Machine Learning/Bayesian-Logistic-Regression/</id>
    <published>2016-03-26T03:30:54.000Z</published>
    <updated>2016-03-29T16:55:42.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Logistic_Regression公式">Logistic Regression公式</h2><blockquote>
<p><code>Logistic Regression</code>（下面简称<code>LR</code>）是一个二分类的机器学习方法，给定一个输入向量$x_i$，输出$P(y_i|x_i)$,其中$y_i \in {0,1}$。</p>
</blockquote>
<p>作为一个二分类问题，$Y$的后验概率一般会写成这样:<br>$$P(Y=1|X)=\frac{1}{1+exp(- \omega - \sum_{i=1}^n {\omega_ix_i})}=\sigma(W^TX_i)$$<br>那么<br><a id="more"></a><br>$$P(Y=0|X)=1-\sigma(W^TX_i)$$</p>
<p>其中$\sigma(\cdot)$表示激活函数,为$S$形状，<code>x</code>轴可以取值无限大，<code>y</code>轴只能取到$(-1,1)$<br>$$\sigma(a)=\frac{1}{1+exp(-a)}$$</p>
<pre><code>由于LR表示简单，训练预测速度快，效果并不是很差<span class="comment">(加上正则化)</span>，所以深得学术和工业界的囍爱~^_^
</code></pre><h2 id="使用GNB推导">使用GNB推导</h2><blockquote>
<p>谈到LR的时候第一印象就是上面的公式，但是为啥是这个公式呢？这一小节就是从<code>GNB(Gaussion Navie Bayes)</code>的角度来看待这个问题~</p>
</blockquote>
<p>我们先对<code>GNB</code>模型做4个假设:</p>
<ol>
<li>$Y$是布尔值，服从伯努利分布，其中$\pi = P(Y=1)$</li>
<li>其中$X_i$是连续随机变量</li>
<li>对于每个$X_i$，$P(X_i|Y=y_k)$服从高斯分布$N(\mu_{ik},\sigma_i)$(大多数情况下，简单用的$N(\mu_k,\sigma)$)</li>
<li>在给定$Y$下，$X_i$与$X_j$条件独立</li>
</ol>
<p>现在让$P(Y|X)$服从<code>GBN</code>假设，通常根据贝叶斯公式可以得到以下:<br>$$P(Y=1|X)=\frac{P(X|Y=1)P(Y=1)}{P(Y=1)P(X|Y=1)+P(Y=0)P(X|Y=0)}$$</p>
<p>再对这个式子进行进一步处理:</p>
<p>$$<br>\begin{equation}\begin{split} P(Y=1|X)&amp;=\frac{1}{1+\frac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)}}\quad\quad &amp;(1)\\<br>&amp;=\frac{1}{1+exp \left(ln\frac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)}\right)}&amp;(2)\\<br>&amp;=\frac{1}{1+exp \left(ln\frac{P(Y=0)}{P(Y=1)}+\sum_iln\frac{P(x_i|Y=0)}{P(x_i|Y=1)}\right)}\quad\quad&amp;(3)\\<br>&amp;=\frac{1}{1+exp \left(ln\frac{1-\pi}{\pi}+\sum_iln\frac{P(x_i|Y=0)}{P(x_i|Y=1)}\right)} &amp;(4)<br>\end{split}\end{equation}<br>$$</p>
<p>其中:</p>
<ol>
<li>式子<code>(1)-&gt;(2)</code>是加了<code>exp</code>函数与<code>ln</code>函数正好相互抵消</li>
<li>式子<code>(2)-&gt;(3)</code>首先将<code>ln</code>函数的相乘转为相加，同时由于$X$中的各个$x_i$相互独立，所以原本写成连乘的式子又可以写成相加求和</li>
<li>式子<code>(3)-&gt;(4)</code>中$P(Y=1)$的概率是$\pi$，则$P(Y=0)$的概率是$1-\pi$</li>
</ol>
<p>在给定假设<code>3</code>情况下，对$\sum_iln\frac{P(x_i|Y=0)}{P(x_i|Y=1)}$进行进一步展开:<br>$$\begin{equation}\begin{split} \sum_iln\frac{P(x_i|Y=0)}{P(x_i|Y=1)} &amp;=\sum_iln\frac{\frac{1}{\sqrt{2\pi\sigma}}exp(\frac{-(x_i-\mu_{i0})^2}{2\sigma_i^2})}{\frac{1}{\sqrt{2\pi\sigma}}exp(\frac{-(x_i-\mu_{i1})^2}{2\sigma_i^2})}  &amp;(5)\\<br>&amp;= \sum_iln  exp\left(\frac{(x_i-\mu_{i1})^2-(x_i-\mu_{i0})^2}{2\sigma_i^2}\right) &amp;(6)\\<br>&amp;= \sum_i \left(\frac{(x_i^2-2x_i\mu_{i1}+\mu_{i1}^2)-(x_i^2-2x_i\mu_{i0}+\mu_{i0}^2)}{2\sigma_i^2}\right) \quad\quad &amp;(7)\\<br>&amp;= \sum_i \left(\frac{2x_i(\mu_{i0}-\mu{i1})+\mu_{i1}^2-\mu_{i0}^2}{2\sigma_i^2}\right) &amp;(8)\\<br>&amp;= \sum_i \left(\frac{\mu_{i0}-\mu_{i1}}{\sigma_i^2}x_i+\frac{\mu_{i1}^2-\mu_{i0}^2}{2\sigma_i^2}\right) &amp;(9)<br>\end{split}\end{equation}$$</p>
<ol>
<li>式子<code>(5)</code>根据假设<code>3</code>而得到，它是服从高斯分布</li>
<li>式子<code>(5)-&gt;(6)</code>是消除了公共因此，并且将指数上的相除转为了相减</li>
<li>式子<code>(6)-&gt;(7)</code>是对<code>ln</code>和<code>exp</code>进行了相互抵消，并且对其平方公式进行了展开</li>
<li>式子<code>(7)-&gt;(8)</code>是展开式中除去了公有的变量</li>
<li>式子<code>(8)-&gt;(9)</code>将$x_i$显眼得提了出来</li>
</ol>
<p>现从新将上面的展开式丢到$P(Y=1|X)$中则可以得到<br>$$P(Y=1|X)=\frac{1}{1+exp \left(ln\frac{1-\pi}{\pi}+\sum_i(\frac{\mu_{i0}-\mu_{i1}}{\sigma_i^2}x_i+\frac{\mu_{i1}^2-\mu_{i0}^2}{2\sigma_i^2})\right)}$$</p>
<p>相应地，则可以将其写为:<br>$$P(Y=1|X)=\frac{1}{1+exp (\omega_0+\sum_i\omega_ix_i)}$$</p>
<blockquote>
<p>可以发现这个式子就是<code>LR</code>的式子了</p>
</blockquote>
<p>其权重$\{\omega_1…\omega_n\}$为<br>$$\omega_i=\frac{\mu_{i0}-\mu_{i1}}{\sigma_i^2}$$<br>其偏置$\omega_0$为:<br>$$\omega_0=ln\frac{1-\pi}{\pi}+\sum_i\frac{\mu_{i1}^2-\mu_{i0}^2}{2\sigma_i^2}$$</p>
<h2 id="总结">总结</h2><p>文本是学习了从贝叶斯角度来看<code>LR</code>式子的来源，根据大家熟知的朴素贝叶斯公式，将定其特定类别下的特征符合高斯分布，根据贝叶斯公式一步步推导出了<code>LR</code>式子的样纸，还是很神奇的。^_^</p>
<h2 id="参考">参考</h2><p>1 <a href="http://web.cse.ohio-state.edu/~kulis/teaching/788_sp12/scribe_notes/lecture6.pdf" target="_blank" rel="external">http://web.cse.ohio-state.edu/~kulis/teaching/788_sp12/scribe_notes/lecture6.pdf</a>(基本就是看了这个，不过里面公式有不少笔误的。。)</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Logistic_Regression公式">Logistic Regression公式</h2><blockquote>
<p><code>Logistic Regression</code>（下面简称<code>LR</code>）是一个二分类的机器学习方法，给定一个输入向量$x_i$，输出$P(y_i|x_i)$,其中$y_i \in {0,1}$。</p>
</blockquote>
<p>作为一个二分类问题，$Y$的后验概率一般会写成这样:<br>$$P(Y=1|X)=\frac{1}{1+exp(- \omega - \sum_{i=1}^n {\omega_ix_i})}=\sigma(W^TX_i)$$<br>那么<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop Streaming导入自定义module]]></title>
    <link href="http://kubicode.me/2016/03/25/Hadoop/Hadoop-Streaming-Import-custom-module/"/>
    <id>http://kubicode.me/2016/03/25/Hadoop/Hadoop-Streaming-Import-custom-module/</id>
    <published>2016-03-24T16:17:54.000Z</published>
    <updated>2016-03-25T01:51:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="问题">问题</h2><p>今天发现用<code>Python</code>编写<code>Hadoop Streaming</code>脚本时，如果自己导入自定义的模块会报错-_-<br>列如<code>word count</code>中的<a href="http://kubicode.me/2015/11/08/Hadoop/Hadoop-Streaming-Primary-Learning-And-Debug/" target="_blank" rel="external">reducer</a>程序:<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#-*- coding=utf8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> utils_helper</span><br><span class="line"></span><br><span class="line">lastk = <span class="keyword">None</span> <span class="comment">#这里标志最后一个k  用于控制同一个key 到一个组中</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">        w,c = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        c = int(c) <span class="comment">#不转成int会比较麻烦  这是是计数</span></span><br><span class="line">        <span class="keyword">if</span> lastk == <span class="keyword">None</span>: <span class="comment">#这里是判断是否过来的是第一个key</span></span><br><span class="line">                lastk=w</span><br><span class="line">                count = utils_helper.add(count,c)</span><br><span class="line">        <span class="keyword">elif</span> lastk == w:</span><br><span class="line">                count += c</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"%s\t%s"</span>%(lastk,count)</span><br><span class="line">                lastk=w</span><br><span class="line">                count = c <span class="comment">#这里重置计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> lastk <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"%s\t%s"</span>%(lastk,count)</span><br></pre></td></tr></table></figure></p>
<p>故意使用一个自定义模块来测试<code>utils_helper.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin/env python</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x,y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x+y</span><br></pre></td></tr></table></figure></p>
<p>如果本地跑起来是(就是本地DEBUG)就可以正常跑的，但是放到<code>Hadoop</code>集群上跑的时候,使用的启动命令为:</p>
<pre><code>hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar \
-<span class="ruby">input /yyl/data/line.txt \
</span>-<span class="ruby">output /yyl/test/ouput/streaming2 \
</span>-<span class="ruby">mapper <span class="string">"python word_count_mapper.py"</span> \
</span>-<span class="ruby">reducer <span class="string">"python word_count_reducer.py"</span> \
</span>-<span class="ruby">file <span class="variable">$HADOOP_HOME</span>/runjar/pyscript/word_count_mapper.py \
</span>-<span class="ruby">file <span class="variable">$HADOOP_HOME</span>/runjar/pyscript/word_count_reducer.py \
</span>-<span class="ruby">file <span class="variable">$HADOOP_HOME</span>/runjar/pyscript/utils_helper.py \</span>
</code></pre><p>可以发现跑到<code>reducer</code>阶段时会报错:</p>
<pre><code><span class="number">16</span>/<span class="number">03</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">11</span>:<span class="number">40</span> INFO mapreduce<span class="class">.Job</span>: Running job: job_1458827745768_0018
<span class="number">16</span>/<span class="number">03</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">11</span>:<span class="number">53</span> INFO mapreduce<span class="class">.Job</span>: Job job_1458827745768_0018 running <span class="keyword">in</span> uber mode : false
<span class="number">16</span>/<span class="number">03</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">11</span>:<span class="number">53</span> INFO mapreduce<span class="class">.Job</span>:  map <span class="number">0%</span> reduce <span class="number">0%</span>
<span class="number">16</span>/<span class="number">03</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">12</span> INFO mapreduce<span class="class">.Job</span>:  map <span class="number">100%</span> reduce <span class="number">0%</span>
<span class="number">16</span>/<span class="number">03</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">22</span> INFO mapreduce<span class="class">.Job</span>: Task Id : attempt_1458827745768_0018_r_000000_0, Status : FAILED
Error: java<span class="class">.lang</span><span class="class">.RuntimeException</span>: PipeMapRed.<span class="function"><span class="title">waitOutputThreads</span><span class="params">()</span></span>: subprocess failed with <span class="tag">code</span> <span class="number">2</span>
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.streaming</span><span class="class">.PipeMapRed</span><span class="class">.waitOutputThreads</span>(PipeMapRed<span class="class">.java</span>:<span class="number">322</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.streaming</span><span class="class">.PipeMapRed</span><span class="class">.mapRedFinished</span>(PipeMapRed<span class="class">.java</span>:<span class="number">535</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.streaming</span><span class="class">.PipeReducer</span><span class="class">.close</span>(PipeReducer<span class="class">.java</span>:<span class="number">134</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.io</span><span class="class">.IOUtils</span><span class="class">.cleanup</span>(IOUtils<span class="class">.java</span>:<span class="number">244</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.ReduceTask</span><span class="class">.runOldReducer</span>(ReduceTask<span class="class">.java</span>:<span class="number">459</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.ReduceTask</span><span class="class">.run</span>(ReduceTask<span class="class">.java</span>:<span class="number">392</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.YarnChild</span>$<span class="number">2</span>.<span class="function"><span class="title">run</span><span class="params">(YarnChild.java:<span class="number">163</span>)</span></span>
    at java<span class="class">.security</span><span class="class">.AccessController</span><span class="class">.doPrivileged</span>(Native Method)
    at javax<span class="class">.security</span><span class="class">.auth</span><span class="class">.Subject</span><span class="class">.doAs</span>(Subject<span class="class">.java</span>:<span class="number">415</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.security</span><span class="class">.UserGroupInformation</span><span class="class">.doAs</span>(UserGroupInformation<span class="class">.java</span>:<span class="number">1657</span>)
    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.YarnChild</span><span class="class">.main</span>(YarnChild<span class="class">.java</span>:<span class="number">158</span>)
</code></pre><p>这就疼了，代码应该没问题呀，尝试了好几遍之后还是这个错误。。。-_-!!</p>
<h2 id="解决方案">解决方案</h2><p>后来在<code>stackoverflow</code>发现有人问了同样的问题，并且我使用其中一个方案解决了:</p>
<pre><code>When Hadoop-Streaming starts <span class="keyword">the</span> python scripts, your python <span class="keyword">script</span>'s path <span class="keyword">is</span> <span class="keyword">where</span> <span class="keyword">the</span> <span class="keyword">script</span> <span class="type">file</span> really <span class="keyword">is</span>. However, hadoop starts them <span class="keyword">at</span> './', <span class="keyword">and</span> your lib.py(<span class="keyword">it</span>'s a symlink) <span class="keyword">is</span> <span class="keyword">at</span> './', too. So, <span class="keyword">try</span> <span class="keyword">to</span> add 'sys.path.append(<span class="string">"./"</span>)' <span class="keyword">before</span> you import lib.py like this: 
import sys
sys.path.append('./')
import lib
</code></pre><blockquote>
<p><code>lib.py</code>表示自定义包</p>
</blockquote>
<p>应该就是<code>-file</code>上传到计算机器之后文件路径的问题产生的，不过感觉他的理由有点疑惑，按他说的如果我上传之后会通过软连接组织到同一目录下再使用，所以如果直接导入包可能会出问题，那我如果上传之前就是在同一目录下应该就不会出问题吧？？这里并不是很理解，但是至少是导入包的问题是解决了^_^</p>
<h2 id="参考">参考</h2><ol>
<li><a href="http://stackoverflow.com/questions/18150208/how-to-import-a-custom-module-in-a-mapreduce-job" target="_blank" rel="external">http://stackoverflow.com/questions/18150208/how-to-import-a-custom-module-in-a-mapreduce-job</a></li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="问题">问题</h2><p>今天发现用<code>Python</code>编写<code>Hadoop Streaming</code>脚本时，如果自己导入自定义的模块会报错-_-<br>列如<code>word count</code>中的<a href="http://kubicode.me/2015/11/08/Hadoop/Hadoop-Streaming-Primary-Learning-And-Debug/">reducer</a>程序:<br>]]>
    
    </summary>
    
      <category term="Hadoop" scheme="http://kubicode.me/tags/Hadoop/"/>
    
      <category term="Hadoop" scheme="http://kubicode.me/categories/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[小技巧]让Hexo在使用Mathjax时支持多行公式]]></title>
    <link href="http://kubicode.me/2016/03/18/Hexo/The-Trick-about-Hexo-Support-MutliLine-Equation-using-Mathjax/"/>
    <id>http://kubicode.me/2016/03/18/Hexo/The-Trick-about-Hexo-Support-MutliLine-Equation-using-Mathjax/</id>
    <published>2016-03-18T01:46:24.000Z</published>
    <updated>2016-03-28T15:42:26.000Z</updated>
    <content type="html"><![CDATA[<p>还是在<code>Hexo</code>中使用<code>Mathjax</code>写<code>Latex</code>公式的问题，在需要些多行的公式的时候，<br>例如:</p>
<pre><code><span class="command">\begin</span><span class="special">{</span>equation<span class="special">}</span><span class="command">\begin</span><span class="special">{</span>split<span class="special">}</span> a<span class="special">&amp;</span>=b+c-d<span class="command">\\</span>
<span class="special">&amp;</span><span class="command">\quad</span> +e-f<span class="command">\\</span>
<span class="special">&amp;</span>=g+h<span class="command">\\</span>
<span class="special">&amp;</span> =i 
<span class="command">\end</span><span class="special">{</span>split<span class="special">}</span><span class="command">\end</span><span class="special">{</span>equation<span class="special">}</span>
</code></pre><p>其中:</p>
<ol>
<li><code>begin</code>和<code>end</code>表示公式的起始</li>
<li><code>\\</code>符号表示换行</li>
<li><code>&amp;</code>表示对齐</li>
</ol>
<a id="more"></a>
<p>结果渲染到html页面之后结果是这样的:<br><img src="/img/The-Trick-about-Hexo-Support-MutliLine-Equation-using-Mathjax/error.png" with="500px"></p>
<p>完全没换行啊，而且又有莫名其妙的空格，按照之前的经验，估计是<code>markdown</code>渲染的<code>html</code>的时候出了问题<br><img src="/img/The-Trick-about-Hexo-Support-MutliLine-Equation-using-Mathjax/error_code.png"></p>
<p>发现两个问题:</p>
<ol>
<li><code>&amp;</code>符号被转义成了<code>&amp;amp;</code></li>
<li>双反斜杠<code>\\</code>被转义成功了<code>\</code></li>
</ol>
<p>这就是公式没换行的原因，肯定是<code>marked.js</code>里面做了处理，不过仔细看<code>Mathjax</code>脚本的配置项中有一项为<code>processEscapes: true</code>，说明<code>MathJax</code>是支持转义符号的，所以类似<code>&amp;amp;</code>是不需要额外处理的。</p>
<p>那么压力就到了解反斜杠问题，最粗暴的是讲反斜杠的转义从<code>marked.js</code>里面去掉，但是可能会影响其他功能，既然两根反斜杠是转为一根，而<code>Latex</code>是两个换行，最简单的方法就是写4个反斜杠:</p>
<pre><code><span class="command">\begin</span><span class="special">{</span>equation<span class="special">}</span><span class="command">\begin</span><span class="special">{</span>split<span class="special">}</span> a<span class="special">&amp;</span>=b+c-d<span class="command">\\</span><span class="command">\\</span>
<span class="special">&amp;</span><span class="command">\quad</span> +e-f<span class="command">\\</span><span class="command">\\</span>
<span class="special">&amp;</span>=g+h<span class="command">\\</span><span class="command">\\</span>
<span class="special">&amp;</span> =i 
<span class="command">\end</span><span class="special">{</span>split<span class="special">}</span><span class="command">\end</span><span class="special">{</span>equation<span class="special">}</span>
</code></pre><p>就可以得到期待的结果了:</p>
<center><img src="/img/The-Trick-about-Hexo-Support-MutliLine-Equation-using-Mathjax/right.png"></center>

<p>这种处理就不影响<code>Hexo</code>自身的功能，又可以满足多行公式的书写^_^</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<p>还是在<code>Hexo</code>中使用<code>Mathjax</code>写<code>Latex</code>公式的问题，在需要些多行的公式的时候，<br>例如:</p>
<pre><code><span class="command">\begin</span><span class="special">{</span>equation<span class="special">}</span><span class="command">\begin</span><span class="special">{</span>split<span class="special">}</span> a<span class="special">&amp;</span>=b+c-d<span class="command">\\</span>
<span class="special">&amp;</span><span class="command">\quad</span> +e-f<span class="command">\\</span>
<span class="special">&amp;</span>=g+h<span class="command">\\</span>
<span class="special">&amp;</span> =i 
<span class="command">\end</span><span class="special">{</span>split<span class="special">}</span><span class="command">\end</span><span class="special">{</span>equation<span class="special">}</span>
</code></pre><p>其中:</p>
<ol>
<li><code>begin</code>和<code>end</code>表示公式的起始</li>
<li><code>\\</code>符号表示换行</li>
<li><code>&amp;</code>表示对齐</li>
</ol>]]>
    
    </summary>
    
      <category term="Hexo" scheme="http://kubicode.me/tags/Hexo/"/>
    
      <category term="Hexo" scheme="http://kubicode.me/categories/Hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[修复Hexo写Mathjax公式多个下标失效的问题]]></title>
    <link href="http://kubicode.me/2016/03/16/Hexo/Fix-Hexo-Bug-In-Mathjax/"/>
    <id>http://kubicode.me/2016/03/16/Hexo/Fix-Hexo-Bug-In-Mathjax/</id>
    <published>2016-03-16T15:25:29.000Z</published>
    <updated>2016-03-28T01:47:42.000Z</updated>
    <content type="html"><![CDATA[<pre><code>这应该严格意义上不算Hexo的bug，但是在写Mathjax的时候就会踩中-_-
</code></pre><blockquote>
<p>说起<code>Markdown</code>写文章时，加粗的第一反应是<code>**</code>，斜体的第一反应是<code>*</code>，因为各种<code>Markdown</code>格式规范的文章里面都是这么教的，但是你不知道的是<code>__</code>可以支持粗体，<code>_</code>可以支持斜体，一般而言这是没什么问题，但是当在写<code>Latex</code>（<code>Hexo</code>里使用<code>Mathjax</code>实现）数据公式时，<code>_</code>表示下标，并且使用频率很高，当一行里面有多个<code>_</code>出现时，<code>Hexo</code>进行解析导致所期待的公式失效。</p>
</blockquote>
<p>自从用<code>Hexo</code>写数学公式的时候，就发现一点小问题，公式复杂了，在<code>Hexo</code>里面就不<code>work</code>，起初以为是<code>Mathjax</code>的支持不完善的缘故，后来发现用了<code>Mathjax</code>的其他博客里面都可以写复杂的公式，而今天又遇到了这个问题：<br>我的公式文本是:<br><code>对于每个$X_i$，$P(X_i|Y=y_k)$服从高斯分布$N(\mu_{ik},\sigma_i)$</code><br>结果生成页面查看之后却发现:<br><a id="more"></a></p>
<p><img src="/img/Fix-Hexo-Bug-In-Mathjax/error_display.png" width="500px"><br>这完全不是我期待的。。。数据公式完全没呈现，并且还变斜体了..<br>最初怀疑是不是公式写错了，结果每个公式去<a href="http://latex.codecogs.com/eqneditor/editor.php" target="_blank" rel="external">在线Latex编辑器</a>里面测试之后都是通过的。。。 这难道<code>Mathjax</code>又不支持了吗？？</p>
<p>趁着周末，得把这个问题解决一把~搞个单页再引用<code>Mathjax</code>之后上面的公式是<code>work</code>的，看<code>Hexo</code>里面渲染的html惊奇的发现:<br><img src="/img/Fix-Hexo-Bug-In-Mathjax/error_code.png" width="500px"><br>下划线<code>_</code>被渲染成<code>&lt;em&gt;</code>标签了，难怪<code>Mathjax</code>公式无法呈现了，<br>接下来<code>Hexo</code>的<code>Markdown</code>渲染引擎:</p>
<pre><code>yans-MacBook-Pro:node_modules yanyl$ grep -r \<span class="variable">&lt;em\&gt;</span> .
./hexo/node_modules/bunyan/docs/bunyan.<span class="number">1</span>.html:<span class="variable">&lt;em&gt;</span>names<span class="variable">&lt;/em&gt;</span> or numeric values. (See 'Log Levels' below.)<span class="variable">&lt;/p&gt;</span><span class="variable">&lt;/dd&gt;</span>
./hexo/node_modules/bunyan/docs/bunyan.<span class="number">1</span>.html:<span class="variable">&lt;dt&gt;</span><span class="variable">&lt;code&gt;</span>-L<span class="variable">&lt;/code&gt;</span>, <span class="variable">&lt;code&gt;</span>--time local<span class="variable">&lt;/code&gt;</span><span class="variable">&lt;/dt&gt;</span><span class="variable">&lt;dd&gt;</span><span class="variable">&lt;p&gt;</span>Display the time field <span class="keyword">in</span> <span class="variable">&lt;em&gt;</span>local<span class="variable">&lt;/em&gt;</span> time, rather than the <span class="keyword">default</span> UTC
./hexo-renderer-marked/node_modules/marked/lib/marked.js:  return '<span class="variable">&lt;em&gt;</span>' + text + '<span class="variable">&lt;/em&gt;</span>';
./hexo-renderer-marked/node_modules/marked/marked.<span class="keyword">min</span>.js:(function(){var <span class="built_in">block</span>={newline:/^\n+
</code></pre><blockquote>
<p>最后一个<code>marked.min.js</code>因为是单行的，所以后面的不贴了</p>
</blockquote>
<p><code>&lt;em&gt;</code>标签的渲染应该就在<code>marked.js</code>或者<code>marked.min.js</code>中，</p>
<pre><code>yans-MacBook-<span class="string">Pro:</span>node_modules yanyl$ grep -r <span class="string">"marked.js"</span> .
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span>bower.<span class="string">json:</span>  <span class="string">"main"</span>: <span class="string">"lib/marked.js"</span>,
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span>component.<span class="string">json:</span>  <span class="string">"scripts"</span>: [<span class="string">"lib/marked.js"</span>],
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span>component.<span class="string">json:</span>  <span class="string">"main"</span>: <span class="string">"lib/marked.js"</span>,
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="string">Makefile:</span>    <span class="annotation">@cp</span> lib/marked.js marked.js
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="string">Makefile:</span>    <span class="annotation">@uglifyjs</span> --comments <span class="string">'/\*[^\0]+?Copyright[^\0]+?\*/'</span> -o marked.min.js lib/marked.js
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="string">Makefile:</span>    <span class="annotation">@rm</span> marked.js
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span>man/marked.1:.TH marked <span class="number">1</span> <span class="string">"2014-01-31"</span> <span class="string">"v0.3.1"</span> <span class="string">"marked.js"</span>
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="keyword">package</span>.<span class="string">json:</span>  <span class="string">"main"</span>: <span class="string">"./lib/marked.js"</span>,
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span>README.<span class="string">md:</span>  &lt;script src=<span class="string">"lib/marked.js"</span>&gt;&lt;/script&gt;
yans-MacBook-<span class="string">Pro:</span>node_modules yanyl$ grep -r <span class="string">"marked.min.js"</span> .
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="string">Makefile:</span>    <span class="annotation">@uglifyjs</span> --comments <span class="string">'/\*[^\0]+?Copyright[^\0]+?\*/'</span> -o marked.min.js lib/marked.js
.<span class="regexp">/hexo-renderer-marked/</span>node_modules<span class="regexp">/marked/</span><span class="string">Makefile:</span>    <span class="annotation">@rm</span> marked.min.js
</code></pre><p>进一步查找可以发现<code>marked.min.js</code>是<code>marked.js</code>的一个压缩版本，并无其他的模块使用，那么进入<code>marked.js</code>中，可以找到<code>&lt;em&gt;</code>的渲染规则:<br><img src="/img/Fix-Hexo-Bug-In-Mathjax/fix1.png" width="500px"><br>和<br><img src="/img/Fix-Hexo-Bug-In-Mathjax/fix2.png" width="500px"></p>
<p>可以发现<code>&lt;em&gt;</code>标签除了<code>*</code>可以渲染，<code>_</code>同样也可以渲染，那么这样就通了<br>解决问题最方便的方法就是关于<code>_</code>渲染，直接将<code>|</code>左侧的<code>_</code>正则匹配删除即可</p>
<p>然后再重新生成页面，刷新可以发现：<br><img src="/img/Fix-Hexo-Bug-In-Mathjax/correct_display.png" width="500px"><br>终于得到期望的结果了</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>这应该严格意义上不算Hexo的bug，但是在写Mathjax的时候就会踩中-_-
</code></pre><blockquote>
<p>说起<code>Markdown</code>写文章时，加粗的第一反应是<code>**</code>，斜体的第一反应是<code>*</code>，因为各种<code>Markdown</code>格式规范的文章里面都是这么教的，但是你不知道的是<code>__</code>可以支持粗体，<code>_</code>可以支持斜体，一般而言这是没什么问题，但是当在写<code>Latex</code>（<code>Hexo</code>里使用<code>Mathjax</code>实现）数据公式时，<code>_</code>表示下标，并且使用频率很高，当一行里面有多个<code>_</code>出现时，<code>Hexo</code>进行解析导致所期待的公式失效。</p>
</blockquote>
<p>自从用<code>Hexo</code>写数学公式的时候，就发现一点小问题，公式复杂了，在<code>Hexo</code>里面就不<code>work</code>，起初以为是<code>Mathjax</code>的支持不完善的缘故，后来发现用了<code>Mathjax</code>的其他博客里面都可以写复杂的公式，而今天又遇到了这个问题：<br>我的公式文本是:<br><code>对于每个$X_i$，$P(X_i|Y=y_k)$服从高斯分布$N(\mu_{ik},\sigma_i)$</code><br>结果生成页面查看之后却发现:<br>]]>
    
    </summary>
    
      <category term="Hexo" scheme="http://kubicode.me/tags/Hexo/"/>
    
      <category term="Hexo" scheme="http://kubicode.me/categories/Hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在信息检索中Term之间的Proximity计算研究]]></title>
    <link href="http://kubicode.me/2016/02/23/Search%20Engine/Proximity-Measures-In-Information-Retrieval/"/>
    <id>http://kubicode.me/2016/02/23/Search Engine/Proximity-Measures-In-Information-Retrieval/</id>
    <published>2016-02-23T12:58:43.000Z</published>
    <updated>2016-03-23T07:28:54.000Z</updated>
    <content type="html"><![CDATA[<h2 id="为啥要做Proximity计算">为啥要做Proximity计算</h2><p>先来看下信息检索/搜索引擎 的一般架构流程:</p>
<ol>
<li>对<code>Doc</code>进行分词,这些分词也叫做<code>Term</code>，然后离线做各种计算</li>
<li>将这些<code>Term</code>灌入倒排索引中</li>
<li>用户查询</li>
<li>根据倒排召回命中<code>Term</code>的文档</li>
<li>将文档根据各个<code>Term</code>算分排序</li>
</ol>
<p>其实可以发现这里查的<code>Term</code> 都是<code>bag-of-words</code>的形式，并且第五步的算法也一般是在线的，所以基本不会做全文扫描之类的事情，那么这样的话问题就来了：<br><a id="more"></a></p>
<pre><code>如果搜索“红色连衣裙”,则可能会出现下面的文档:
<span class="number">1.</span>xxxx红色连衣裙xxxx
<span class="number">2.</span>红色高跟鞋配连衣裙
很明显文档<span class="number">1</span>的相关性比<span class="number">2</span>要高，但是此时如果仅仅是bag-<span class="keyword">of</span>-<span class="property">words</span>模型就很难保证<span class="number">1</span>的相关性分要比<span class="number">2</span>高
</code></pre><p>所以一般的搜索引擎还有一个叫做<code>Proximity Measures</code>的特征计算，可以理解为计算文档里面出现的<code>query Term</code>的相近程度，为了保证可行性，降低计算的复杂度，一般也只会计算两个<code>Term</code>之间的<code>Proximity</code>分</p>
<h2 id="使用距离度量">使用距离度量</h2><p>这种方式主要是计算<code>Term</code>之间距离作为<code>Proximity</code>得分，主要分两大类:</p>
<ol>
<li><code>Span-based</code>:使用时将全部的<code>query term</code>丢进去一起算距离</li>
<li><code>Distance aggregation</code>:先算两两之间的距离，再聚集起来</li>
</ol>
<p>假设现在有文档<code>D = t1, t2, t1, t3, t5, t4, t2, t3, t4</code>，基于<code>D</code>集合来讲讲各个距离的计算方式<br><strong>Span-based</strong></p>
<ul>
<li><code>Span</code>:在文档中可以覆盖所有term的最小距离称为<code>Span</code>，<strong>需要包含所有重复的term</strong><br>  比如$Q=t1,t2$这个查询的$Span=7$</li>
<li><code>MinCover</code>:在文档中可以覆盖所有term的最小距离称为<code>MinCover</code>,<strong>每个term至少被包含一次</strong><br>  比如这里的$Q=t1,t2$查询的$MinCover=1$</li>
</ul>
<p><strong>Distance aggregation</strong></p>
<blockquote>
<p>这种方式计算的最近单元是计算一个term pair的最小距离，使用$Dis(t_i,t_j;D)$来表示</p>
</blockquote>
<ul>
<li><code>MinDist(Minimum pair distance)</code>:计算所有pair的最小距离的最小值,<br>  <center>$MinDist=min_{q_1,q_2 \in Q \cap D,q_1 \neq q_2} Dis(q_1,q_2;D)$</center><br>比如$Q={t1,t2,t3}$，则$MinDist=min(1,2,3)=1$</li>
<li><code>AveDist(Average pair distance)</code>:计算所有pair的最小距离的平均值，<br>  <center>$AveDist=\frac{2}{n \cdot (n+1)}min_{q_1,q_2 \in Q \cap D,q_1 \neq q_2} Dis(q_1,q_2;D)$</center><br>比如$Q={t1,t2,t3}$，则$AveDist=(1+2+3)/3=2$</li>
<li><code>MaxDist(Maximum pair distance)</code>:与<code>MinDist</code>正好相反，它是求最大值<br>  <center>$MinDist=max_{q_1,q_2 \in Q \cap D,q_1 \neq q_2} Dis(q_1,q_2;D)$</center><br>  比如$Q={t1,t2,t3}$，则$MaxDist=max(1,2,3)=3$</li>
</ul>
<p>文献中实验表明:</p>
<ol>
<li><code>Span-based</code>为考虑到各个文档长度，以各自文档的长度最为归一化因子进行归一化之后效果要好一些</li>
<li><code>Distance aggregation</code>系列一般比<code>Span-based</code>的效果要好</li>
<li><code>Distance aggregation</code>中<code>MinDist</code>的效果最好</li>
</ol>
<p>但是在一般使用过程中不会直接将距离作为<code>Proximity</code>的值，现将$\delta(Q,D)$作为查询词在各个文档的中的距离度量，$\delta(Q,D)$最小表明查询词与文档越相关，而在使用过程中一般以这个相关性越大最好，这将这个相关性记为:$\pi(Q,D)$，则使用下面的公式来转换:<br>$$\pi(Q,D)=log(\alpha + exp(- \delta(Q,D)))$$</p>
<blockquote>
<p>$\alpha$可以作为调节因子</p>
</blockquote>
<p>使用这种方式的度量最大的优点就是方便，但是单独用起来效果可能不怎么理解，并且波动性较大.~</p>
<h2 id="引入BM25模型">引入BM25模型</h2><p>主要对<code>bi-term</code>进行<code>BM25</code>得分的计算，这里<code>BM25</code>的计算方式可以按传统的进行，<a href="http://kubicode.me/2016/01/26/Search%20Engine/Study-BM25-For-Query-Document-Relevance/" target="_blank" rel="external">参考这个</a> </p>
<p>关于<code>bi-term</code>的构建主要有两种方式:</p>
<ol>
<li>直接使用<code>B-Gram</code>:认为相邻两个<code>term</code>是有依赖的，所以可以直接使用<code>B-Gram</code>的方式来构建</li>
<li>使用滑窗的方式:认为一个窗口里面的两两<code>term</code>有依赖，因此可以对他们进行两两组合，这个窗口大小一般会小于8</li>
</ol>
<blockquote>
<p>其实更好的应该是<code>依存</code>关系来构建这个<code>bi-term</code>，不过上述的几种方式构建出来的<code>pair</code>都会很大，所以还需要其他一些方式来剪枝</p>
</blockquote>
<h1 id="持续研究中~~~">持续研究中~~~</h1><h2 id="参考">参考</h2><ol>
<li>2007-An Exploration of Proximity Measures in Information Retrieval</li>
<li>2005-A Markov random field model for term dependencies</li>
<li>2010-How good is a span of terms?: exploiting proximity to improve web retrieval</li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="为啥要做Proximity计算">为啥要做Proximity计算</h2><p>先来看下信息检索/搜索引擎 的一般架构流程:</p>
<ol>
<li>对<code>Doc</code>进行分词,这些分词也叫做<code>Term</code>，然后离线做各种计算</li>
<li>将这些<code>Term</code>灌入倒排索引中</li>
<li>用户查询</li>
<li>根据倒排召回命中<code>Term</code>的文档</li>
<li>将文档根据各个<code>Term</code>算分排序</li>
</ol>
<p>其实可以发现这里查的<code>Term</code> 都是<code>bag-of-words</code>的形式，并且第五步的算法也一般是在线的，所以基本不会做全文扫描之类的事情，那么这样的话问题就来了：<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/categories/Search-Engine/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[针对BM25遇到长文档时失效情况的一种高效解决方案]]></title>
    <link href="http://kubicode.me/2016/02/16/Search%20Engine/BM25-Fails-When-The-Docments-Are-Very-Long/"/>
    <id>http://kubicode.me/2016/02/16/Search Engine/BM25-Fails-When-The-Docments-Are-Very-Long/</id>
    <published>2016-02-16T09:17:19.000Z</published>
    <updated>2016-02-27T06:58:10.000Z</updated>
    <content type="html"><![CDATA[<pre><code>B<span class="title">M25</span>在长文档下会失效，文本是记录SIGIR上的一个Paper的解决方案~
</code></pre><h2 id="当BM25遇到长文档">当BM25遇到长文档</h2><p>文档相关性模型-<code>BM25</code>的拟合公式如下:</p>
<center>$\sum_{i\in Q} log \frac {(r_i+0.5)((N-R)-(n_i-r_i)+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)} \cdot \frac{(k_1+1)f_i}{K+f_i} \cdot \frac{(k_2+1)qf_i}{k_2+qf_i}$</center>

<p>其中第一部分表示<code>BIM</code>的值，第二部分表示在文档中的权重，第三部分表示在查询词中的权重，(具体符号解释参考之前的<a href="http://kubicode.me/2016/01/26/Search%20Engine/Study-BM25-For-Query-Document-Relevance/" target="_blank" rel="external">BM25介绍</a>)现将第二部分单独拿出来:</p>
<p><center>$f(q,D)=\frac{(k_1+1) \times TF}{k_1 \times ((1-b)+b \cdot \frac{dl}{avdl} )+TF}=\frac{(k_1+1) \times c’(q,D)}{k_1+c’(q,D)}$</center><br>其中:<br><a id="more"></a></p>
<p><center>$c’(q,D)=\frac{TF}{1-b+b \cdot \frac{dl}{avdl}}$</center><br>现在先来做一个假设，假设当前有个文档<code>很长很长</code>，也就是$dl$这个值很大，则可以发现$c’(q,D)$就会很小，小到接近于0，因此会导致文档部分的因子$f(q,D)$也会接近于0，几乎和这个词没有出现在这个文档一样..-_-</p>
<p>在这种情况下，针对长文档，<code>BM25</code>的效果会比较差<br>下面的图就是<code>BM25</code>在不同文档长度下的实验</p>
<p><img src="/img/BM25-Fails-When-The-Docments-Are-Very-Long/doc-len-comp.png" width="600px"></p>
<p>可以发现在<code>BM25</code>中，随着文档的变长，相关性在变高，但是其被检索的概率并没有随着相关性的趋势而变高，也就是长文档使用<code>BM25</code>的效果变得比较差。</p>
<h2 id="高效解决方案">高效解决方案</h2><p>为了避免长文档在<code>BM25</code>的相关性中被惩罚，我们需要对文档权重$f(q,D)$做一个规范化约束，但是由于<code>BM25</code>早已被公认为是一种比较有效的文档相关性，所以这个约束不能破坏掉<code>BM25</code>自身的特征.</p>
<p>我们希望规范化约束之后保持以下三点特性:</p>
<ol>
<li>当$c’(q,D)=0$的时候，$f(q,D)$也为0</li>
<li>随着$c’(q,D)$的变大，$f(q,D)$也需要呈现单调递增，但是会趋向于一个最大值</li>
<li>随着$c’(q,D)$的减少，$f(q,D)$会单调递减趋向于一个最小值，但是这个最小值需要足够大</li>
</ol>
<blockquote>
<p>就是因为原生的<code>BM25</code>不满足第3点，所以会出现在长文档下<code>BM25</code>出现失效的情况</p>
</blockquote>
<!-- 
f'(q,D)=\left\{
\begin{aligned}
\frac{(k_1+1) \cdot [c'(q,D)+\delta ]}{k_1 + [c'(q,D)+\delta ]}  & \quad if \quad c'(q,D)>0\\
0 & \quad otherwise\\
\end{aligned}
\right.
-->
<p>而下面的改进$f’(q,D)$正好可以满足上述三个特性:</p>
<p><img src="/img/BM25-Fails-When-The-Docments-Are-Very-Long/gj.gif" alt=""></p>
<p>使用时针对原来的$c’(q,D)$值增加了一个平滑项$\delta$，增加了平滑项之后依然满足<code>第1、2点</code>特性，并且当$c’(q,D)&gt;0$的时候$f’(q,D)$有一个下界:</p>
<p><center>$\frac{(k_1+1) \cdot \delta}{k_1+ \delta}$</center><br>这样也正好可以满足<code>第3点</code>特性</p>
<p>进行该项小改进之后的模型称为<code>BM25L</code>,遇到长文档时并不会失效，并且还保持原有的<code>BM25</code>特性.</p>
<h2 id="实验效果验证">实验效果验证</h2><p>针对多个数据集  ，使用不同的调节参数$b$,$k_1$,$\delta$进行试验:<br><img src="/img/BM25-Fails-When-The-Docments-Are-Very-Long/exp.png"></p>
<p>在第三章图明显可以发现<code>BM25L</code>的效果对<code>BM25</code>有较大的绝对提升，其中较优的参数范围为:</p>
<ol>
<li>$b \in [0,3,0.6] $</li>
<li>$k_1 \in [1.0,2.0]$</li>
<li>$\delta = 0.5$时最优</li>
</ol>
<h2 id="参考">参考</h2><ol>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.600.16&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">When Documents Are Very Long, BM25 Fails!</a></li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>B<span class="title">M25</span>在长文档下会失效，文本是记录SIGIR上的一个Paper的解决方案~
</code></pre><h2 id="当BM25遇到长文档">当BM25遇到长文档</h2><p>文档相关性模型-<code>BM25</code>的拟合公式如下:</p>
<center>$\sum_{i\in Q} log \frac {(r_i+0.5)((N-R)-(n_i-r_i)+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)} \cdot \frac{(k_1+1)f_i}{K+f_i} \cdot \frac{(k_2+1)qf_i}{k_2+qf_i}$</center>

<p>其中第一部分表示<code>BIM</code>的值，第二部分表示在文档中的权重，第三部分表示在查询词中的权重，(具体符号解释参考之前的<a href="http://kubicode.me/2016/01/26/Search%20Engine/Study-BM25-For-Query-Document-Relevance/">BM25介绍</a>)现将第二部分单独拿出来:</p>
<p><center>$f(q,D)=\frac{(k_1+1) \times TF}{k_1 \times ((1-b)+b \cdot \frac{dl}{avdl} )+TF}=\frac{(k_1+1) \times c’(q,D)}{k_1+c’(q,D)}$</center><br>其中:<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/categories/Search-Engine/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Learning to rank学习基础]]></title>
    <link href="http://kubicode.me/2016/02/15/Machine%20Learning/Learning-To-Rank-Base-Knowledge/"/>
    <id>http://kubicode.me/2016/02/15/Machine Learning/Learning-To-Rank-Base-Knowledge/</id>
    <published>2016-02-15T02:31:43.000Z</published>
    <updated>2016-05-07T07:00:26.000Z</updated>
    <content type="html"><![CDATA[<pre><code>Learning to <span class="function"><span class="title">rank</span><span class="params">(简写 LTR、L2R)</span></span> 也叫排序学习，指的是机器学习中任何用于排序的技术。
</code></pre><h2 id="为什么要用LTR">为什么要用LTR</h2><p>传统的检索模型靠人工拟合排序公式，并通过不断的实验确定最佳的参数组合，以此来形成相关性打分。这种方式非常简单高效，应该范围也很广，比如简单的博客排序、论坛的<code>QA</code>排序等.但是也同时存在较大的问题:</p>
<ol>
<li>手动调参工作量太大</li>
<li>可能会过拟合</li>
<li>如果模型参数很多，手动调参的可用性就很低了~</li>
</ol>
<p>LTR与此思路不同，最合理的排序公式由机器学习算法来确定，而人则需要给机器学习提供训练数据,他的优势有:</p>
<ol>
<li>可以自动调节参数</li>
<li>可以融合多方面观点的(evidences)的数据</li>
<li>避免过拟合(通过正则项)</li>
</ol>
<a id="more"></a>
<h2 id="LTR基本框架">LTR基本框架</h2><p><code>LTR</code>的核心还是机器学习，只是目标不仅仅是简单的分类或者回归了，最主要的是产出文档的排序结果，它通常的工作框架如下:<br><img src="/img/Learning-To-Rank-Base-Knowledge/lrt_framework.png" width="400px"></p>
<p>所描述的步骤为:<code>训练数据获取-&gt;特征提取-&gt;模型训练-&gt;测试数据预测-&gt;效果评估</code></p>
<h2 id="训练数据的获取">训练数据的获取</h2><h4 id="人工标注">人工标注</h4><p>人工标注的数据主要有以下几大类型:</p>
<ul>
<li><p>单点标注</p>
<ul>
<li>对于每个查询文档打上绝对标签</li>
<li>二元标注：相关 vs 不相关</li>
<li><p>五级标注：完美(Perfect),出色(Excellent),好(Good),一般(Fair),差(Bad) ，一般后面两档属于不相关</p>
<blockquote>
<p>好处：标注的量少O(n)<br>坏处：难标。。。不好统一</p>
</blockquote>
</li>
</ul>
</li>
<li><p>两两标注</p>
<ul>
<li><p>对于一个查询<code>Query</code>,要标注文档$d1$比文档$d2$是否更加相关 $(q,d1)\succ (q,d2)?$</p>
<blockquote>
<p>好处：标注起来比较方便<br>坏处：标注量大 估计得有O(n^2)</p>
</blockquote>
</li>
</ul>
</li>
<li><p>列表标注</p>
<ul>
<li><p>对于一个查询<code>Query</code>，将人工理想的排序整个儿标好</p>
<blockquote>
<p>好处： 相对于上面两种，标的效果会很好<br>坏处： 这个工作量也太大了…-_-||</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="日志抽取">日志抽取</h4><p>当搜索引擎搭建起来之后用户的点击数据就变得非常好使。</p>
<pre><code>比如，结果ABC分别位于<span class="number">123</span>位，B比<span class="literal">A</span>位置低，但却得到了更多的点击，那么B的相关性可能好于<span class="literal">A</span>.
</code></pre><p>这种点击数据隐含了<code>Query</code>到文档的相关性好坏。所以一般会使用点击倒置的<code>高低位</code>结果作为训练数据.</p>
<p>但是他也是存在问题的：</p>
<ul>
<li>用户总是习惯于从上到下浏览搜索结果</li>
<li>用户点击有比较大的噪声</li>
<li>一般头查询(<code>head query</code>)才存在用户点击</li>
</ul>
<h2 id="特征提取">特征提取</h2><p>检索系统会使用一系列特征来表示一次查询，通过模型之后最终决定文档的排序顺序，这里用$q$来表示查询,$d$表示查询的文档,$occur-term$表示$q$与$d$共现的词，则提取的特征主要有以下三大类:</p>
<ul>
<li>$occur-term$与$q$特征<ul>
<li>共现在查询中的出现次数、比率等</li>
</ul>
</li>
<li>$occur-term$与$d$的特征<ul>
<li>共现在文档中的出现次数、比率等</li>
<li>共现词与文档的相关性特征:<code>BM25系列</code></li>
</ul>
</li>
<li>$d$自身特征<ul>
<li><code>PageRank</code>值</li>
<li><code>Spam</code>信息</li>
<li><code>Quality</code>质量分</li>
<li>行为分,<code>ctr</code>，<code>停留时间</code>，<code>二跳率</code>等..</li>
</ul>
</li>
</ul>
<h2 id="模型训练">模型训练</h2><p><code>LTR</code>的模型主要有单文档方法(<code>Pointwise Approach</code>)、文档对方法(<code>Pairwise Approach</code>)和列表方法(<code>Listwise Approach</code>)三大类,下面是实现他们的各种算法:</p>
<h3 id="Pointwise_Approach">Pointwise Approach</h3><pre><code><span class="variable">Pointwise</span>的处理对象是单独的一篇文档，将文档转换为特征向量之后，机器学习模型根据从训练数据中学习到的分类或者回归函数对文档打分，打分的结果就是搜索的结果.
</code></pre><p>其实就是将文档排序转为了文档的回归、分类和有序分类问题，其函数的框架为:</p>
<center>$L(F(X),y)=\sum_{i=1}^{n}l(f(x_i)-y_i)$</center><br>输入:<br>- 单个文档查询对:$(x_i,y_i)$<br>- 完全忽略上下文的关系<br>- 将标注转为数字，比如<code>Perfect-&gt;5, Excellent-&gt;4, Good-&gt;3, Fair-&gt;2, Bad-&gt;1</code><br><br>输出:<br>- 排序函数,对于给定查询文档对,能够计算出得分(score)<br><br>关于<code>Pointwise</code>下的三个分支，这张图解释的很好:<br><center><img src="/img/Learning-To-Rank-Base-Knowledge/pointwise_flow.png" width="400px"></center>

<p>其主要区别就是<code>loss function</code>不同，也就是最后的结果目标不一样:</p>
<ul>
<li><code>Classification</code>:输入的是5档类别（作为离散），最后通过分类模型输测试数据的各个档位档位的概率，然后进行加权成一个实数值</li>
<li><code>Regression</code>:输入的是5档或者2档（作为连续），最后通过回归模型输出测试数据的一个相关性实数(就是回归)</li>
<li><code>Ordinal Classification</code>:输入有序的档位，一般2档(离散),最后通过分类模型的阈值给到一个档位(应该是二分类把)</li>
</ul>
<p>实现<code>Pointwise</code>方法的主要算法有:</p>
<ul>
<li>Classification<ul>
<li>Discriminative model for IR (SIGIR 2004)</li>
<li><strong>McRank (NIPS 2007)</strong></li>
</ul>
</li>
<li>Regression<ul>
<li>Least Square Retrieval Function (TOIS 1989)</li>
<li>Regression Tree for Ordinal Class Prediction (Fundamenta Informaticae, 2000)</li>
<li><strong>Subset Ranking using Regression (COLT 2006)</strong></li>
</ul>
</li>
<li>Ordinal Classification<ul>
<li><strong>Pranking (NIPS 2002)</strong></li>
<li>OAP-BPM (EMCL 2003)</li>
<li>Ranking with Large Margin Principles (NIPS 2002)</li>
<li>Constraint Ordinal Regression (ICML 2005)</li>
</ul>
</li>
</ul>
<p><strong>优点:</strong></p>
<ul>
<li>速度快，复杂度低.</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li>效果一般</li>
<li>没有考虑到文档之间的相对关系</li>
<li>忽略了文档相关度与查询的关联，比如<code>Top Query</code>排在后面的相关性比<code>Tial Query</code>排在前面的都要高，导致训练样本不一致</li>
</ul>
<h3 id="Pairwise_Approach">Pairwise Approach</h3><pre><code>对于搜索任务来说，系统接收到用户查询后，返回相关文档列表，所以问题的关键是确定文档之间的先后相对顺序关系，而<span class="variable">Pairwise</span>则将重点转向对文档关系是否合理的判断.
</code></pre><p><code>Pairwise</code>主要是讲排序问题转为了文档对顺序的判断<br>以下图为例:<br><img src="/img/Learning-To-Rank-Base-Knowledge/pairwise_example.png" width="500px"><br>对于查询<code>Q1</code>进行人工标注之后,<code>Doc2=5</code>的分数最高，其次是<code>Doc3</code>为4分，最差的是<code>Doc1</code>为3分，将此转为相对关系之后有:<code>Doc2&gt;Doc1、Doc2&gt;Doc3、Doc3&gt;Doc1</code>，而根据这个顺序关系逆向也可以得到相关性的排序顺序，所以排序问题可以很自然的转为任意两个文档关系的判断,而任意两个文档顺序的判断就称为了一个很熟悉的分类问题.<br><code>Pairwise</code>的函数框架为:</p>
<!-- $L(F(x),y)=\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(sign(y_i-y_j),f(x_i)-f(x_j))$ -->
<center><img src="/img/Learning-To-Rank-Base-Knowledge/pairwise_func.gif"></center>

<p><strong>输入</strong>:</p>
<ul>
<li>同一查询的一对文档$(x_i,x_j,sign(y_i-y_j))$</li>
<li>标注两个文档的相对关系，如果文档$x_i$比$x_j$更加相关，则$sign(y_i-y_j))=1$</li>
<li>分布保留同一查询下的文档间关系</li>
</ul>
<p><strong>输出</strong>:</p>
<ul>
<li>排序函数给出文档对的计算得分</li>
</ul>
<p>关于<code>Pairwise</code>最终的算分，其实分类和回归都可以实现:</p>
<center><img src="/img/Learning-To-Rank-Base-Knowledge/pairwise_flow.png" width="400px"></center>


<p>实现<code>Pairwise Approach</code>方法的主要算法有:</p>
<ul>
<li>Learning to Retrieve Information (SCC 1995)</li>
<li>Learning to Order Things (NIPS 1998)</li>
<li><strong>Ranking SVM (ICANN 1999)</strong></li>
<li><strong>RankBoost (JMLR 2003)</strong></li>
<li>LDM (SIGIR 2005)</li>
<li><strong>RankNet (ICML 2005)</strong></li>
<li><strong>Frank (SIGIR 2007)</strong></li>
<li>MHR(SIGIR 2007)</li>
<li><strong>GBRank (SIGIR 2007)</strong></li>
<li>QBRank (NIPS 2007)</li>
<li>MPRank (ICML 2007)</li>
<li><strong>IRSVM (SIGIR 2006)</strong></li>
<li>LambdaRank (NIPS 2006)</li>
</ul>
<p>虽然<code>Pairwise</code>方法对<code>Pointwise</code>方法做了改进，但是也明显存在两个问题:</p>
<ol>
<li>只考虑了两个文档的先后顺序，没有考虑文档出现在搜索列表中的位置</li>
<li>不同的查询，其相关文档数量差异很大，转换为文档对之后，有的查询可能有几百对文档，有的可能只有几十个，最终对机器学习的效果评价造成困难</li>
</ol>
<h3 id="Listwise_Approach">Listwise Approach</h3><pre><code>与<span class="variable">Pointwise</span>和<span class="variable">Pairwise</span>不同，<span class="variable">Listwise</span>是将一个查询对应的所有搜索结果列表作为一个训练实例，因此也称为文档列方法.
</code></pre><p>文档列方法根据$K$个训练实例训练得到最优的评分函数$F$，对于一个新的查询，函数$F$对每一个文档进行打分，之后按照得分顺序高低排序，就是对应的搜索结果.<br><code>Listwise</code>主要有两类:</p>
<ul>
<li><code>Measure specific</code>:损失函数与评估指标相关,比如:$L(F(x),y)=exp(-NDCG)$</li>
<li><code>Non-measure specific</code>:损失函数与评估指标不是显示相关,考虑了信息检索中的一些独特性质</li>
</ul>
<p>实现<code>Listwise</code>的算法主要有:</p>
<ul>
<li>Measure-specific<ul>
<li><strong>AdaRank (SIGIR 2007)</strong></li>
<li><strong>SVM-MAP (SIGIR 2007)</strong></li>
<li><strong>SoftRank (LR4IR 2007)</strong></li>
<li>RankGP (LR4IR 2007)</li>
<li>LambdaMART (inf.retr 2010)</li>
</ul>
</li>
<li>Non-measure specific<ul>
<li><strong>ListNet (ICML 2007)</strong></li>
<li><strong>ListMLE (ICML 2008)</strong></li>
<li>BoltzRank (ICML 2009)</li>
</ul>
</li>
</ul>
<blockquote>
<p>实验表明 一般<code>Listwise</code>要好于前两种排序算法，但是其复杂度是在太高了</p>
</blockquote>
<h3 id="方法对比">方法对比</h3><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">pointwise</th>
<th style="text-align:left">pairwise</th>
<th style="text-align:left">listwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">输入信息的完整度</td>
<td style="text-align:left"><strong>不完全</strong></td>
<td style="text-align:left"><strong>部分完全</strong></td>
<td style="text-align:left"><strong>完全</strong></td>
</tr>
<tr>
<td style="text-align:left">输入</td>
<td style="text-align:left">$(x,y)$</td>
<td style="text-align:left">$(x_1,x_2,y)$</td>
<td style="text-align:left">$(x_1,x_2…x_n,\pi)$</td>
</tr>
<tr>
<td style="text-align:left">输出</td>
<td style="text-align:left">$f(x)$</td>
<td style="text-align:left">$f(x)$</td>
<td style="text-align:left">$f(x)$</td>
</tr>
<tr>
<td style="text-align:left">样本复杂度</td>
<td style="text-align:left">$O(n)$</td>
<td style="text-align:left">O(n^2)</td>
<td style="text-align:left">O(n!)</td>
</tr>
<tr>
<td style="text-align:left">表现</td>
<td style="text-align:left"><em>差</em></td>
<td style="text-align:left"><em>中</em></td>
<td style="text-align:left"><em>好</em></td>
</tr>
</tbody>
</table>
<h2 id="评估指标">评估指标</h2><h3 id="MAP">MAP</h3><p><code>MAP</code>(<code>Mean Average Precision</code>)表示文档在排序中的平均精度均值，是用于衡量个query查询见过的平均精度值<code>AP</code>,<br>系统检索出来的相关文档越靠前(rank 越高)，MAP就可能越高。如果系统没有返回相关文档，则准确率默认为0。</p>
<p>所以对于计算<code>AP</code>的值就是关键了,<code>MAP</code>对文档只分为两档:相关与不相关，则<code>AP</code>表示为</p>
<center>$AP=\frac{\sum_{i:relvance}^{n}P_i/R_i}{n}$</center>

<blockquote>
<p>其中$P_i$表示所有召回文档中相关文档的相对次序，$R_i$表示所有被召回的相关文档中在所有文档中的次序</p>
</blockquote>
<p>比如在某个<code>query</code>得到的排序中:<br><img src="/img/Learning-To-Rank-Base-Knowledge/map_demo.png" width="400px"><br>则他的相对顺序的表格为:</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">O</th>
<th style="text-align:left">X</th>
<th style="text-align:left">O</th>
<th style="text-align:left">O</th>
<th style="text-align:left">O</th>
<th style="text-align:left">O</th>
<th style="text-align:left">X</th>
<th style="text-align:left">X</th>
<th style="text-align:left">X</th>
<th style="text-align:left">O</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">相关文档次序</td>
<td style="text-align:left">1</td>
<td style="text-align:left"></td>
<td style="text-align:left">2</td>
<td style="text-align:left">3</td>
<td style="text-align:left">4</td>
<td style="text-align:left">5</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left">6</td>
</tr>
<tr>
<td style="text-align:left">所有召回文档次序</td>
<td style="text-align:left">1</td>
<td style="text-align:left">2</td>
<td style="text-align:left">3</td>
<td style="text-align:left">4</td>
<td style="text-align:left">5</td>
<td style="text-align:left">6</td>
<td style="text-align:left">7</td>
<td style="text-align:left">8</td>
<td style="text-align:left">9</td>
<td style="text-align:left">10</td>
</tr>
<tr>
<td style="text-align:left">Precision</td>
<td style="text-align:left">1/1</td>
<td style="text-align:left"></td>
<td style="text-align:left">2/3</td>
<td style="text-align:left">3/4</td>
<td style="text-align:left">4/5</td>
<td style="text-align:left">5/6</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left">6/10</td>
</tr>
</tbody>
</table>
<p>最终计算的$AP=(1/1+2/3+3/4+4/5+5/6+6/10)/6=0.78$</p>
<h3 id="NDCG">NDCG</h3><p><code>NDCG</code>表示表示归一化折损累积增益，主要是衡量实际相关性越高的文档排在越前面，它的全称是<code>Normalized Discounted Cumulative Gain</code>,也正好代表了4个部分的含义:</p>
<ul>
<li><p><code>Gain</code>:表示增益，一般相关性越高，增益值也是越大</p>
  <center>$G_i=2^{y_i}-1$</center>

<blockquote>
<p>其中$y_i$表示文档相关性档位，一般档位越高，相关性越大</p>
</blockquote>
</li>
<li><p><code>Discounted</code>:一般认为排序位置 带来的权重不同，所以会有一个折损因子</p>
  <center>$DG_i=\frac{2^{y_i}-1}{log(1+i)}$</center>

<blockquote>
<p>其中$i$为文档的排序位置，从1开始</p>
</blockquote>
</li>
<li><p><code>Cumulative</code>：表示在一次<code>query</code>查询中所有的增益累加</p>
  <center>$DCG=\sum_{i=1}^{n}{\frac{2^{y_i}-1}{log(1+i)}}$</center>
</li>
<li><p><code>Normalized</code>：为归一化，因为在不同的查询中可能<code>DCG</code>的值波动较大，这里计算各个query最理想的排序的<code>DCG</code>值作为归一化因子，也称为<code>IDCG</code></p>
  <center>$NDCG=\frac{1}{IDCG} \cdot \sum_{i=1}^{n}{\frac{2^{y_i}-1}{log(1+i)}}$</center>

</li>
</ul>
<p><code>NDCG</code>的可使用性更加广泛了，但是还是存在以下三点限制:</p>
<ol>
<li><code>NDCG</code>并没有对不相关文档进行惩罚</li>
<li><code>NDCG</code>对一些缺失的完成结果也没有进行惩罚</li>
<li><code>NDCG</code>也不是用档位大家都相等的情况（比如每页里面的doc相关性都是差不多的）</li>
</ol>
<h2 id="公开数据集">公开数据集</h2><ol>
<li><a href="http://research.microsoft.com/en-us/um/beijing/projects/letor/" target="_blank" rel="external">http://research.microsoft.com/en-us/um/beijing/projects/letor/</a></li>
<li><a href="http://research.microsoft.com/en-us/projects/mslr/" target="_blank" rel="external">http://research.microsoft.com/en-us/projects/mslr/</a></li>
<li><a href="http://webscope.sandbox.yahoo.com/" target="_blank" rel="external">http://webscope.sandbox.yahoo.com/</a></li>
</ol>
<h2 id="总结">总结</h2><p>在玩搜索引擎时敲定特征分的权重是非常疼的一件事儿，而<code>LTR</code>正好帮你定分，<code>LTR</code>的三种实现方法其实各有优劣：</p>
<ol>
<li>难点主要在训练样本的构建(人工或者挖日志)，另外就是训练复杂</li>
<li>虽说<code>Listwise</code>效果最好，但是天下武功唯快不破，看看这篇文章<a href="http://www.cnblogs.com/zjgtan/p/3652689.html" target="_blank" rel="external">http://www.cnblogs.com/zjgtan/p/3652689.html</a>体验下</li>
<li>在工业界用的比较多的应该还是<code>Pairwise</code>，因为他构建训练样本相对方便，并且复杂度也还可以，所以<code>Rank SVM</code>就很火啊^_^</li>
</ol>
<h2 id="参考">参考</h2><ul>
<li>《这就是搜索引擎：核心技术详解》 . 第五章</li>
<li><a href="http://blog.crackcell.com/posts/2011/12/17/a_short_intro_2_ltr.html" target="_blank" rel="external">Learning to Rank小结</a></li>
<li><a href="http://www.icst.pku.edu.cn/lcwm/course/WebDataMining/slides2012/8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.pdf" target="_blank" rel="external">机器学习及排序学习基础</a></li>
<li><a href="http://www.contem.org/2010summer/slides/LEARNING%20TO%20RANK%20TUTORIAL%20-%20tyliu.pdf" target="_blank" rel="external">Learning to Rank for Information Retrieval</a></li>
<li><a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain" target="_blank" rel="external">https://en.wikipedia.org/wiki/Discounted_cumulative_gain</a></li>
<li><a href="https://www.kaggle.com/wiki/MeanAveragePrecision" target="_blank" rel="external">https://www.kaggle.com/wiki/MeanAveragePrecision</a></li>
</ul>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>Learning to <span class="function"><span class="title">rank</span><span class="params">(简写 LTR、L2R)</span></span> 也叫排序学习，指的是机器学习中任何用于排序的技术。
</code></pre><h2 id="为什么要用LTR">为什么要用LTR</h2><p>传统的检索模型靠人工拟合排序公式，并通过不断的实验确定最佳的参数组合，以此来形成相关性打分。这种方式非常简单高效，应该范围也很广，比如简单的博客排序、论坛的<code>QA</code>排序等.但是也同时存在较大的问题:</p>
<ol>
<li>手动调参工作量太大</li>
<li>可能会过拟合</li>
<li>如果模型参数很多，手动调参的可用性就很低了~</li>
</ol>
<p>LTR与此思路不同，最合理的排序公式由机器学习算法来确定，而人则需要给机器学习提供训练数据,他的优势有:</p>
<ol>
<li>可以自动调节参数</li>
<li>可以融合多方面观点的(evidences)的数据</li>
<li>避免过拟合(通过正则项)</li>
</ol>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Machine Learning" scheme="http://kubicode.me/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[求二进制中1的个数]]></title>
    <link href="http://kubicode.me/2016/02/12/Algorithm/The-One-Occur-in-Bitnum/"/>
    <id>http://kubicode.me/2016/02/12/Algorithm/The-One-Occur-in-Bitnum/</id>
    <published>2016-02-12T12:26:05.000Z</published>
    <updated>2016-04-12T15:20:11.000Z</updated>
    <content type="html"><![CDATA[<h2 id="题目">题目</h2><pre><code>求一个二进制数字中1出现的个数
</code></pre><blockquote>
<p>这道题目面试非常常见，但是工作中也很实用，以32位int整形为例，下面列举一下一些经典的做法(主要是做记录用^_^)</p>
</blockquote>
<h2 id="遍历法">遍历法</h2><p>这个是最简单的方法，向右移位，判断最低位是否为1进行计数即可!</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="javadoc">/**</span><br><span class="line"> * 普通遍历法</span><br><span class="line"> *<span class="javadoctag"> @param</span> n</span><br><span class="line"> *<span class="javadoctag"> @return</span></span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">traverseCount</span><span class="params">(<span class="keyword">int</span> n)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(n&gt;<span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		n = n&gt;&gt;<span class="number">1</span>;</span><br><span class="line">		count++;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>该方法最直观，不过复杂度是<code>O(N)</code></p>
<h2 id="快速查找法">快速查找法</h2><p>为啥说快速呢，因为这里的复杂度是和1个个数有关，有多少1就循环多少次，该方法的设计也非常巧妙<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="javadoc">/**</span><br><span class="line"> * 快速计数法</span><br><span class="line"> *<span class="javadoctag"> @param</span> n</span><br><span class="line"> *<span class="javadoctag"> @return</span></span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">quickCount</span><span class="params">(<span class="keyword">int</span> n)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(n&gt;<span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		n &amp;= (n-<span class="number">1</span>);</span><br><span class="line">		count ++;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>大致可以这么理解</p>
<pre><code>假设当前的<span class="keyword">n</span>=7
<span class="keyword">n</span>          <span class="keyword">n</span>-1
0111    0110
0110    0101
0100    0000
</code></pre><p>复杂度是<code>O(K)</code>,其中<code>K</code>为二进制数中1个数，非常实用</p>
<h2 id="查表法">查表法</h2><p>一般整形的位数是固定的，假如我们知道每种情况下的1的个数，其实直接查表即可，但是如果是32位的整形枚举这个表的可能性不是很大，但是巧妙的方法是可以将其分割成4个8位的数字，而8位的数字就只有256种情况了，可以直接方法预设的表中<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="javadoc">/**</span><br><span class="line"> * 查表法</span><br><span class="line"> *<span class="javadoctag"> @param</span> n</span><br><span class="line"> *<span class="javadoctag"> @return</span></span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">searchTableCount</span><span class="params">(<span class="keyword">int</span> n)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span>[] table = </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, </span><br><span class="line">        <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, </span><br><span class="line">        <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, </span><br><span class="line">        <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, </span><br><span class="line">        <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, </span><br><span class="line">        <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, </span><br><span class="line">        <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, </span><br><span class="line">        <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, </span><br><span class="line">    &#125;; </span><br><span class="line">	<span class="comment">//直接查表得到</span></span><br><span class="line">	<span class="keyword">return</span> table[n&amp;<span class="number">0xFF</span>] + table[(n&gt;&gt;<span class="number">8</span>)&amp;<span class="number">0xFF</span>] +table[(n&gt;&gt;<span class="number">16</span>)&amp;<span class="number">0xFF</span>] +table[(n&gt;&gt;<span class="number">24</span>)&amp;<span class="number">0xFF</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>该方法的复杂度是<code>O(N/8)</code>,占的空间也不是特别多，也是比较实用的一种</p>
<p>还有一些神奇的方法可以看<a href="http://www.cnblogs.com/graphics/archive/2010/06/21/1752421.html" target="_blank" rel="external">http://www.cnblogs.com/graphics/archive/2010/06/21/1752421.html</a>,写得非常赞</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="题目">题目</h2><pre><code>求一个二进制数字中1出现的个数
</code></pre><blockquote>
<p>这道题目面试非常常见，但是工作中也很实用，以32位int整形为例，下面列举一下一些经典的做法(主要是做记录用^_^)</p>
</blockquote>
<h2 id="遍历法">遍历法</h2><p>这个是最简单的方法，向右移位，判断最低位是否为1进行计数即可!</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="javadoc">/**</span><br><span class="line"> * 普通遍历法</span><br><span class="line"> *<span class="javadoctag"> @param</span> n</span><br><span class="line"> *<span class="javadoctag"> @return</span></span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">traverseCount</span><span class="params">(<span class="keyword">int</span> n)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(n&gt;<span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		n = n&gt;&gt;<span class="number">1</span>;</span><br><span class="line">		count++;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="Algorithm" scheme="http://kubicode.me/tags/Algorithm/"/>
    
      <category term="Algorithm" scheme="http://kubicode.me/categories/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在Hexo中离线安装数据工具包-Mathjax]]></title>
    <link href="http://kubicode.me/2016/01/27/Hexo/Offline-Install-Mathjax-In-Hexo-Jacman/"/>
    <id>http://kubicode.me/2016/01/27/Hexo/Offline-Install-Mathjax-In-Hexo-Jacman/</id>
    <published>2016-01-27T09:30:11.000Z</published>
    <updated>2016-02-03T06:24:54.000Z</updated>
    <content type="html"><![CDATA[<pre><code>这里与其说安装，还是不如直接拷贝相应文件到hexo目录
</code></pre><p>最近尝试在写<code>blog</code>的时候用<code>Latex</code>来写数学公式，写起来还是极其方便灵活滴，但是，但是其速度慢如爬蜗牛，一般修改一下，在刷新一下要<code>4~5s</code>才能将公式渲染出来，其书写效率也太低了，总之我不能忍，所以准备将其<code>MathJax</code>放在本地.</p>
<ol>
<li>一方面是希望加快公式的渲染速度</li>
<li>另一方面也是希望能支持我在断网的情况下也能正常使用<code>Mathjax</code></li>
</ol>
<a id="more"></a>
<p>我用的是<a href="http://wuchong.me/" target="_blank" rel="external">Jark</a>大神的<code>jacman</code>主题，在简单<code>grep</code>查找之后可以发现<code>MathJax</code>的引用在:<br><code>/themes/jacman/layout/_partial/mathjax.ejs</code>这里，可以发现也就远程引用了官网cdn的一个js<br><code>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;</code></p>
<p>在<a href="http://docs.mathjax.org/en/latest/installation.html" target="_blank" rel="external">http://docs.mathjax.org/en/latest/installation.html</a>可以下载到<code>Mathjax</code>的离线包（建议乖乖直接下包，别去爬<code>cdn</code>里面的目录，心累）。</p>
<p>离线包解压之后的<code>unpacked</code>目录里面有整理好直接引用的资源文件(尼玛，之类有<code>16m</code>，难怪慢如蜗牛..)</p>
<p>在<code>/themes/jacman/source/js</code>新建一个<code>mathjax/2.5-latest</code>的目录，将<code>unpacked</code>下的文件整个儿拷贝过去,修改<code>mathjax.ejs</code>中的引用<br><code>&lt;script type=&quot;text/javascript&quot; src=&quot;/js/mathjax/2.5-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;</code></p>
<p>清空目录，再重新生成文件,启动服务器，跑起来看公式 只能说   <code>如丝般顺滑</code> ^_^</p>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>这里与其说安装，还是不如直接拷贝相应文件到hexo目录
</code></pre><p>最近尝试在写<code>blog</code>的时候用<code>Latex</code>来写数学公式，写起来还是极其方便灵活滴，但是，但是其速度慢如爬蜗牛，一般修改一下，在刷新一下要<code>4~5s</code>才能将公式渲染出来，其书写效率也太低了，总之我不能忍，所以准备将其<code>MathJax</code>放在本地.</p>
<ol>
<li>一方面是希望加快公式的渲染速度</li>
<li>另一方面也是希望能支持我在断网的情况下也能正常使用<code>Mathjax</code></li>
</ol>]]>
    
    </summary>
    
      <category term="Hexo" scheme="http://kubicode.me/tags/Hexo/"/>
    
      <category term="Hexo" scheme="http://kubicode.me/categories/Hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[概率检索模型BM25系列-文档相关性检索的利器]]></title>
    <link href="http://kubicode.me/2016/01/26/Search%20Engine/Study-BM25-For-Query-Document-Relevance/"/>
    <id>http://kubicode.me/2016/01/26/Search Engine/Study-BM25-For-Query-Document-Relevance/</id>
    <published>2016-01-26T09:06:31.000Z</published>
    <updated>2016-04-02T14:20:39.000Z</updated>
    <content type="html"><![CDATA[<pre><code>给定一个用户需求(<span class="keyword">query</span>),如果搜索系统展示的搜索结果是根据文档和<span class="keyword">query</span>的相关性由高向低排序的，那么这个搜索引擎是最优的。在文档集合的基础上计算其相关性估计是其核心~
</code></pre><h2 id="概率排序原理">概率排序原理</h2><p>以往的<code>向量空间模型</code>是将<code>query</code>和文档使用向量表示然后计算其内容相似性来进行相关性估计的，而<code>概率检索模型</code>是一种直接对用户需求进行相关性的建模方法,一个<code>query</code>进来，将所有的文档分为两类—-<code>相关文档</code>、<code>不相关文档</code>,这样就转为了一个相关性的分类问题,赞!</p>
<p>对于某个文档$D$来说，$P(R|D)$表示该文档数据相关文档的概率，则$P(NR|D)$表示该文档属于不相关文档的概率，如果<code>query</code>属于相关文档的概率大于不相关文档$P(R|D)&gt;P(RN|D)$，则认为这个文档是与用户查询相关相关的.</p>
<a id="more"></a>
<p>现在使用贝叶斯公式将其转一下:</p>
<center>$P(R|D)&gt;P(NR|D) &lt;=&gt;\frac{P(D|R)P(R)}{P(D)}&gt;\frac{P(D|NR)P(NR)}{P(D)} &lt;=&gt; \frac{P(D|R)}{P(D|NR)}&gt;\frac{P(NR)}{P(R)}$</center>

<p>在搜索排序过程中不需要真正的分类，只需要保证相关性由高到底排序即可，所以只需要$\frac{P(D|R)}{P(D|NR)}$降序即可，<br>这样就最终转为计算$P(D|R)$,$P(D|NR)$的值即可.</p>
<h2 id="二元独立模型(BIM)">二元独立模型(BIM)</h2><pre><code><span class="xml">词汇独立性假设:文档里面出现的词没有任何关联，这样一个文档的出现就可以转为各个单词出现概率的乘积（虽然这种假设有违实际，但是算起来简单的啊</span><span class="keyword">^_</span><span class="xml">^）</span>
</code></pre><p>上述提到的文档$D$表示为<code>{1,0,1,0,1}</code>，用$p_i$来表示第$i$个单词在相关文档出现的概率,则在已知<code>相关文档</code>集合的情况下，观察到$D$的概率为:<br>    <center>$P(D|R)=p_1 \times (1-p_2) \times p_3 \times (1-p_4) \times p_5$ </center></p>
<blockquote>
<p>第<code>1,3,5</code>表示这个单词在$D$中出现,所以其贡献概率为$p_i$，而第<code>2,4</code>这两个单词并没有在$D$中出现，所以其贡献的概率为$1-p_i$ </p>
</blockquote>
<p>同理在<code>不相关文档</code>中观察到的概率为:<br>    <center>$P(D|R)=s_1 \times (1-s_2) \times s_3 \times (1-s_4) \times s_5$ </center></p>
<p>最终得到的相关性概率估算为:</p>
<center>$\frac{P(D|R)}{P(D|NR)}=\frac{p_1 \times (1-p_2) \times p_3 \times (1-p_4) \times p_5}{s_1 \times (1-s_2) \times s_3 \times (1-s_4) \times s_5}$</center>

<p>现在将其推广之后可以有通用的式子：</p>
<p>$$\frac{P(D|R)}{P(D|NR)}= \prod_{i:d_i=1} \frac{p_i}{s_i} \times \prod_{i:d_i=0} \frac{1-p_i}{1-s_i}$$</p>
<blockquote>
<p>$d_i=1$表示在文档中出现的单词，$d_i=0$表示没在文档中出现的单词:</p>
</blockquote>
<p>在这里进一步对上述公式进行等价变换之后有:</p>
<p>$$\begin{equation}\begin{split} \frac{P(D|R)}{P(D|NR)} &amp;=\prod_{i:d_i=1} \frac{p_i}{s_i} \times \left ( \prod_{i:d_i=1} \frac{1-s_i}{1-p_i} \times \prod_{i:d_i=1} \frac{1-p_i}{1-s_i} \right ) \times \prod_{i:d_i=0} \frac{1-p_i}{1-s_i}\\<br>&amp;= \left ( \prod_{i:d_i=1} \frac{p_i}{s_i} \times \prod_{i:d_i=1} \frac{1-s_i}{1-p_i} \right ) \times  \left ( \prod_{i:d_i=1} \frac{1-p_i}{1-s_i} \times \prod_{i:d_i=0} \frac{1-p_i}{1-s_i} \right )  \\<br>&amp;=\prod_{i:d_i=1} \frac{p_i(1-s_i)}{s_i(1-p_i)} \times \prod_i \frac{1-pi}{1-s_i} \\<br>&amp;=\prod_{i:d_i=1} \frac{p_i(1-s_i)}{s_i(1-p_i)}<br>\end{split}\end{equation}$$</p>
<p>其中上面式子第三步的第二部分$\prod_i \frac{1-pi}{1-s_i} $表示各个单词在所有文档中出现的概率，所以这个式子的值和具体文档并没有什么关系，在排序中不起作用，才可以简化到第4步.</p>
<p>为了方便计算，将上述连乘公式取$log$:</p>
<center>$log(\frac{P(D|R)}{P(D|NR)}) =  \sum_{i:d_i=1} log \frac{p_i(1-s_i)}{s_i(1-p_i)}$</center>

<p>有了上述最终可计算的式子之后，我们就只需要统计文档$D$中的各个单词在<code>相关文档</code>/<code>不相关文档</code>中出现的概率即可:</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">相关文档</th>
<th style="text-align:left">不相关文档</th>
<th style="text-align:left">文档数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$d_i=1$</td>
<td style="text-align:left">$r_i$</td>
<td style="text-align:left">$n_i-r_i$</td>
<td style="text-align:left">$n_i$</td>
</tr>
<tr>
<td style="text-align:left">$d_i=0$</td>
<td style="text-align:left">$R-r_i$</td>
<td style="text-align:left">$(N-R)-(n_i-r_i)$</td>
<td style="text-align:left">$N-n_i$</td>
</tr>
<tr>
<td style="text-align:left">文档数量</td>
<td style="text-align:left">$R$</td>
<td style="text-align:left">$N-R$</td>
<td style="text-align:left">$N$</td>
</tr>
</tbody>
</table>
<p>上面的表格表示各个单词在文档集合中的<code>相关文档</code>/<code>不相关文档</code>出现数量,同时为了避免$log(0)$出现，加上平滑之后可以计算得到:</p>
<center>$p_i=\frac{r_i+0.5}{R+1}$</center><br><center>$s_i=\frac{(n_i-r_i)+0.5}{(N-R)+1}$</center>

<p>则最终可以得到如下公式:</p>
<center>$\sum_{q_i=d_i=1} log \frac {(r_i+0.5)((N-R)-(n_i-r_i)+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)}$</center>

<p>上面的公式表示对于同时出现查询$q_i$以及文档$d_i$的时候，对$q_i$在$d_i$中出现的单词在<code>相关文档</code>/<code>不相关文档</code>进行统计，即可得到查询与文档的相关性估计值.</p>
<blockquote>
<p>在不确定哪些文档是相关的，哪些文档是不相关的的时候，可以给公式的估算因子直接赋予固定值，则该公式将会蜕化为$IDF$因子.</p>
</blockquote>
<h2 id="BM25_模型">BM25 模型</h2><h3 id="模型概述">模型概述</h3><blockquote>
<p>上一小节中的<code>BIM模型</code>效果并不佳，也没有考虑单词权重，但是他给<code>BM25模型</code>打下了深深的基础</p>
</blockquote>
<pre><code>B<span class="title">M25</span> 模型在BIM模型的基础上考虑了查询词在Query以及Doc中的权重，并通过实验引入了一些经验参数。B<span class="title">M25</span>模型是目前最成功的内容排序模型.
</code></pre><p>改进之后的<code>BM25</code>模型的拟合公式如下:</p>
<center>$\sum_{i\in Q} log \frac {(r_i+0.5)((N-R)-(n_i-r_i)+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)} \cdot \frac{(k_1+1)f_i}{K+f_i} \cdot \frac{(k_2+1)qf_i}{k_2+qf_i}$</center>

<p>上面的式子中:</p>
<ol>
<li>第1部分即为上一小节的二元独立模型BIM计算得分</li>
<li>第2部分是查询词在$D$中的权重，其中$f_i$代表词在文档中的词频,$K$因子代表了对文档长度的考虑，其计算公式为$K=k_1((1-b)+b \cdot \frac{dl}{avdl})$<ol>
<li>$k_1$为经验参数,这里的$k_1$一般设置为1.2,</li>
<li>$b$为调节因子，将$b$设为0时，文档长度因素将不起作用，经验表明一般$b=0.75$</li>
<li>$dl$代表当前文档的长度</li>
<li>$avdl$代表所有文档的平均长度</li>
</ol>
</li>
<li>第3部分是查询词在自身的权重,$qf_i$表示在查询中的词频,$k_2$也为调节因子，因为在短查询下这部分一般为1，为了放大这部分的差异，$k_2$一般取值为<code>0~1000</code></li>
</ol>
<blockquote>
<p>综合看来,<code>BM25</code>模型结合了<code>BIM</code>因子、<code>文档长度</code>、<code>文档词频</code>和<code>查询词频</code>进行公式融合，并利用$k_1$、$k_2$、$b$对各种因子进行权重的调整.</p>
</blockquote>
<h3 id="栗子">栗子</h3><p>假设当前以<code>乔布斯 IPAD2</code>这个查询词为例，来计算在某文档$D$中<code>BM25相关性</code>的值，由于不知道文档集中相关与不相关的分类，所以这里直接将相关文档个数$r$置为0,则将得到的<code>BIM</code>因子为:<br>    <center>$Rel_{BIM}=log \frac {(0+0.5)((N-0)-(n_i-0)+0.5)}{(n_i-0+0.5)(0-0+0.5)}=log \frac{N-n_i+0.5}{n_i+0.5}$</center></p>
<p>其他数值假定如下:</p>
<ol>
<li>文档的集合总数$N=100000$</li>
<li>包含<code>乔布斯</code>的文档个数为$n_{乔布斯}=1000$</li>
<li>包含<code>IPAD2</code>的文档个数为$n_{IPAD2}=100$</li>
<li>文档$D$中出现<code>乔布斯</code>的词频为$f_{乔布斯}=8$</li>
<li>文档$D$中出现<code>IPAD2</code>的词频为$f_{IPAD2}=8$</li>
<li>查询词频均为$qf_i=1$</li>
<li>调节因子$k_1=1.2$</li>
<li>调节因子$k_2=200$</li>
<li>调节因子$b=0.75$</li>
<li>设文档$D$的长度为平均长度的1.5倍($\frac{dl}{avdl}=1.5$),即$K=1.2 \times (0.25+0.75 \times 1.5)=1.65$</li>
</ol>
<p>则最终可以计算到的<code>BM25</code>结果为:</p>
<p>$Rel_{BM25}=log \frac{100000-1000+0.5}{1000+0.5} \times \frac{(1.2+1) \times 8}{1.65+8} \times \frac{(200+1) \times 1}{200+1}+ log \frac{100000-1000+0.5}{1000+0.5} \times \frac{(1.2+1) \times 5}{1.65+5} \times \frac{(200+1) \times 1}{200+1} = 8.59$</p>
<p>每个文档按上述公式计算得到相关性排序即可.</p>
<h2 id="BM25F_模型">BM25F 模型</h2><blockquote>
<p>在<code>BM25</code>模型中，文档被当做一个整体进行进行词频的统计，而忽视了不同区域的重要性，<code>BM25F</code>模型正是抓住了这点进行了相应的改进。</p>
</blockquote>
<p><code>BM25F</code>模型在计算相关性时候，会对文档分割成不同的域来进行加权统计，非常适用于网页搜索，因为在一个网页有<code>标题信息</code>、<code>meta信息</code>、<code>页面内容信息</code>等，而<code>标题信息</code>无疑是最重要的，其次是<code>meta信息</code>，最后才是<code>网页内容</code>，<code>BM25F</code>在计算相关性的，会将网页分为不用的区域，在各个区域分别统计自己的词频。<br>所以<code>BM25F</code>模型的计算公式为:</p>
<p>$$\sum_{i:q_i=d_i=1} log \frac {(r_i+0.5)((N-R)-(n_i-r_i)+0.5)}{(n_i-r_i+0.5)(R-r_i+0.5)} \times \frac{f_i^u}{k_1+f_i^u}$$</p>
<blockquote>
<p><code>BM25F</code>的第1部分还是<code>BIM</code>的值</p>
</blockquote>
<p>其中与<code>BM25</code>主要的差别体现在$f_i^u$因子上,它是单词$i$在各个区域不同的得分,计算公式如下:</p>
<p>$$f_i^u=\sum_{k=1}^uw_k \times \frac{f_{ui}}{B_u}$$</p>
<p>$$B_u=((1-b_u)+b_u \times  \frac{ul_u}{uvul_u})$$</p>
<p>上面的公式表示:</p>
<ol>
<li>文档$D$来个不同的$u$个域</li>
<li>各个域对应的权重为$W_k$</li>
<li>$f_i^u$为第$i$个单词在各个域中的 $\frac{f_{ui}}{B_u}$ 的加权和</li>
<li>$f_{ui}$表示词频</li>
<li>$B_u$表示各个域的长度情况</li>
<li>$ul_u$为实际域的实际长度,$uvul_u$表示域的平均长度</li>
<li>$b_u$则为各个域长度的调节因子</li>
</ol>
<h2 id="总结">总结</h2><p><code>BM25</code>系列的模型主要是考虑查询词到文档里面的各种词频，含有大量自由的调节因子，最终给出查询到文档的相关性，模型都是比较简单的，但是里面提高的相关文档和不相关文档的引入将比较麻烦，有实际场景可以一试。</p>
<h2 id="参考">参考</h2><ol>
<li>《这就是搜索引擎：核心技术详解》 . 第五章</li>
</ol>
<hr>
<blockquote>
<p>本作品采用<a href="http://creativecommons.org/licenses/by-nc-sa/2.5/cn/" target="_blank">[知识共享署名-非商业性使用-相同方式共享 2.5]</a>中国大陆许可协议进行许可，我的博客欢迎复制共享，但在同时，希望保留我的署名权<a href="http://kubicode.me/" target="_blank" rel="external">kubiCode</a>，并且，不得用于商业用途。如您有任何疑问或者授权方面的协商，请给<a href="http://kubicode.me/about/" target="_blank" rel="external">我留言</a>。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>给定一个用户需求(<span class="keyword">query</span>),如果搜索系统展示的搜索结果是根据文档和<span class="keyword">query</span>的相关性由高向低排序的，那么这个搜索引擎是最优的。在文档集合的基础上计算其相关性估计是其核心~
</code></pre><h2 id="概率排序原理">概率排序原理</h2><p>以往的<code>向量空间模型</code>是将<code>query</code>和文档使用向量表示然后计算其内容相似性来进行相关性估计的，而<code>概率检索模型</code>是一种直接对用户需求进行相关性的建模方法,一个<code>query</code>进来，将所有的文档分为两类—-<code>相关文档</code>、<code>不相关文档</code>,这样就转为了一个相关性的分类问题,赞!</p>
<p>对于某个文档$D$来说，$P(R|D)$表示该文档数据相关文档的概率，则$P(NR|D)$表示该文档属于不相关文档的概率，如果<code>query</code>属于相关文档的概率大于不相关文档$P(R|D)&gt;P(RN|D)$，则认为这个文档是与用户查询相关相关的.</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://kubicode.me/tags/Machine-Learning/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/tags/Search-Engine/"/>
    
      <category term="Search Engine" scheme="http://kubicode.me/categories/Search-Engine/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[用笨方法解决Shell中按行读取文件之后Tab键不见的问题]]></title>
    <link href="http://kubicode.me/2016/01/03/Linux/shell-line-no-tab/"/>
    <id>http://kubicode.me/2016/01/03/Linux/shell-line-no-tab/</id>
    <published>2016-01-03T14:24:09.000Z</published>
    <updated>2016-01-03T15:03:02.000Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p><code>Shell</code>强大灵活，但是今天在使用<code>Shell</code>处理按行读取文件时发现  读取到的行设定的<code>tab</code>分割符不见了</p>
</blockquote>
<p>先看一下演示的待<a href="http://kubicode.me/img/awk-study-list/score.log" target="_blank" rel="external">处理文件</a>:<br><img src="/img/shell-line-no-tab/demo_text.png" alt=""></p>
<p>需要做的需求就是将里面的数据使用<code>shell</code>将第一列读取出来做一些其他处理(当然如果仅仅是完成读取首列这个功能，使用<code>awk</code>是最快的，这里只是演示)</p>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#! /bin/bash</span><br><span class="line"></span></span><br><span class="line">cat score.log | <span class="keyword">while</span> <span class="built_in">read</span> line  <span class="comment">#按行读取文件</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"></span><br><span class="line">    grade=`<span class="built_in">echo</span> <span class="variable">$line</span> | awk -F <span class="string">"\t"</span> <span class="string">'&#123;print $1&#125;'</span>` <span class="comment">#读取首列（年级）</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$grade</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>执行这个文件并且使用管道<code>| vim -</code> 打印只之后可以发现格式不对，按行打印了，并且其原先的分隔符<code>tab</code>也消失了<br><img src="/img/shell-line-no-tab/no_tab.png" alt=""></p>
<p>后来查到据说<code>shell</code>的默认分隔符是空格的原因,所以一种最直接的方式就是去<code>.bashrc</code>里面修改分割符,但是这么做可能会影响其他的<code>shell程序</code><br>所以比较保险的一种方法就是先将行中的<code>tab</code>替换，再对替换后的数据进行操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#! /bin/bash</span><br><span class="line"></span></span><br><span class="line">cat score.log | awk <span class="string">'gsub("\t","\001",$0)'</span> | <span class="keyword">while</span> <span class="built_in">read</span> line  <span class="comment">#按行读取文件</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    grade=`<span class="built_in">echo</span> <span class="variable">$line</span> | awk -F <span class="string">"\001"</span> <span class="string">'&#123;print $1&#125;'</span>` <span class="comment">#读取首列（年级）</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$grade</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>先做了独立的<code>tab</code>去掉之后就可以很顺利地看到需要的结果了</p>
<p><img src="/img/shell-line-no-tab/ok.png" alt=""></p>
<blockquote>
<p>这个方法虽然比较简单，但是非常实用~</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p><code>Shell</code>强大灵活，但是今天在使用<code>Shell</code>处理按行读取文件时发现  读取到的行设定的<code>tab</code>分割符不见了</p>
</blockquote>
<p>先看一下演示的待<a href="http://kubicode.me/img/awk-study-list/score.log">处理文件</a>:<br><img src="/img/shell-line-no-tab/demo_text.png" alt=""></p>
<p>需要做的需求就是将里面的数据使用<code>shell</code>将第一列读取出来做一些其他处理(当然如果仅仅是完成读取首列这个功能，使用<code>awk</code>是最快的，这里只是演示)</p>]]>
    
    </summary>
    
      <category term="Linux" scheme="http://kubicode.me/tags/Linux/"/>
    
      <category term="Vim" scheme="http://kubicode.me/tags/Vim/"/>
    
      <category term="Linux" scheme="http://kubicode.me/categories/Linux/"/>
    
  </entry>
  
</feed>